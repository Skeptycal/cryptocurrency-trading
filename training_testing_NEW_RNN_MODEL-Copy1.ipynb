{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "import lz4.frame\n",
    "import gzip\n",
    "import io\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from plumbum.cmd import rm\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())\n",
    "\n",
    "#from keras import backend as K\n",
    "#print(K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotline(data):\n",
    "    plt.figure()\n",
    "    plt.plot(data)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def event_count(time_series, data_name):\n",
    "    time_series = time_series[['Fill Price (USD)']].values\n",
    "    upevents = 0\n",
    "    downevents = 0\n",
    "    sameprice = 0\n",
    "    prev_obv = time_series[0]\n",
    "    for obv in time_series[1:]:\n",
    "        if obv > prev_obv:\n",
    "            upevents += 1\n",
    "        elif obv < prev_obv:\n",
    "            downevents += 1\n",
    "        elif obv == prev_obv:\n",
    "            sameprice += 1\n",
    "        prev_obv = obv\n",
    "    print('=== Event counts on %s ===' % data_name)\n",
    "    print('upevents')\n",
    "    print(upevents)\n",
    "    print('downevents')\n",
    "    print(downevents)\n",
    "    print('sameprice')\n",
    "    print(sameprice)\n",
    "    print()\n",
    "\n",
    "def mse(time_series, data_name):\n",
    "    time_series = time_series[['Fill Price (USD)']].values\n",
    "    total_squared_error = 0\n",
    "    total_absolute_error = 0\n",
    "    prev_obv = time_series[0]\n",
    "    for obv in time_series[1:]:\n",
    "        total_squared_error += (obv - prev_obv)**2\n",
    "        total_absolute_error += abs(obv - prev_obv)\n",
    "        prev_obv = obv\n",
    "    num_predictions = len(time_series) - 1\n",
    "    mean_squared_error = total_squared_error / num_predictions\n",
    "    mean_absolute_error = total_absolute_error / num_predictions\n",
    "    root_mean_squared_error = np.sqrt(mean_squared_error)\n",
    "    print('=== baseline on %s ===' % data_name)\n",
    "    print('total squared error')\n",
    "    print(total_squared_error)\n",
    "    print('total absolute error')\n",
    "    print(total_absolute_error)\n",
    "    print('mean squared error')\n",
    "    print(mean_squared_error)\n",
    "    print('mean absolute error')\n",
    "    print(mean_absolute_error) \n",
    "    print('root mean squared error')\n",
    "    print(root_mean_squared_error) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_summary_statistics():\n",
    "    #event_count(small_set, 'small')\n",
    "    train_set = df.iloc[0:num_samples_training]\n",
    "    dev_set = df.iloc[num_samples_training:num_samples_training+num_samples_dev]\n",
    "    test_set = df.iloc[num_samples_training+num_samples_dev:]\n",
    "    event_count(train_set, 'train')\n",
    "    event_count(dev_set, 'dev')\n",
    "    event_count(test_set, 'test')\n",
    "    mse(train_set, 'train')\n",
    "    mse(dev_set, 'dev')\n",
    "    mse(test_set, 'test')\n",
    "#show_summary_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    values = np.array(data)\n",
    "    values = values.reshape(-1,1)\n",
    "    values = values.astype('float32') \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(model_history, title):\n",
    "    plt.figure()\n",
    "    plt.plot(model_history.history['loss'], label='Train')\n",
    "    plt.plot(model_history.history['val_loss'], label='Dev')\n",
    "    plt.xlabel('Epochs'); plt.ylabel('Loss (mse)')\n",
    "    plt.title(title)\n",
    "    plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform_pricescaler(data, Y_prevrawprice, fitted_scaler):\n",
    "    return fitted_scaler.inverse_transform(preprocess(data))\n",
    "\n",
    "def inverse_transform_percentdiff(data, Y_prevrawprice, fitted_scaler=None):\n",
    "    orig_prices = Y_prevrawprice\n",
    "    change = orig_prices * data\n",
    "    return orig_prices + change\n",
    "    #return fitted_scaler.inverse_transform(preprocess(data))\n",
    "\n",
    "#print(Y_test_prevrawprice)\n",
    "#print(inverse_transform_percentdiff(Y_test, Y_test_prevrawprice))\n",
    "\n",
    "inverse_transform = inverse_transform_percentdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(model, X_test, Y_test, Y_prevrawprice, title, inverse=False, scaler=None):\n",
    "    y_hat = model.predict(X_test)\n",
    "\n",
    "    if inverse:\n",
    "        y_hat = inverse_transform(y_hat, Y_prevrawprice, scaler)\n",
    "        Y_test = inverse_transform(Y_test, Y_prevrawprice, scaler)\n",
    "\n",
    "    plt.plot(y_hat, label='Predicted')\n",
    "    plt.plot(Y_test, label='True')\n",
    "    plt.xlabel('Time'); \n",
    "\n",
    "    if inverse:\n",
    "        plt.ylabel('Price')\n",
    "    else:\n",
    "        plt.ylabel('RESCALED Price')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MSE_RMSE(model, scaler, X_test, Y_test, Y_prevrawprice, model_name):\n",
    "    y_hat = model.predict(X_test)\n",
    "    y_hat_inverse = inverse_transform(y_hat, Y_prevrawprice, scaler)\n",
    "    Y_test_inverse = inverse_transform(Y_test, Y_prevrawprice, scaler)\n",
    "    mse = mean_squared_error(Y_test_inverse, y_hat_inverse)\n",
    "    rmse = np.sqrt(mean_squared_error(Y_test_inverse, y_hat_inverse))\n",
    "    print('%s:' % model_name)\n",
    "    print('Test MSE: %.3f' % mse)\n",
    "    print('Test RMSE: %.3f' % rmse)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(model, model_name, \n",
    "                   X_train, Y_train, Y_train_prevrawprice, X_dev, Y_dev, Y_dev_prevrawprice, X_test, Y_test, Y_test_prevrawprice,\n",
    "                   lag=10, batch_size=100, epochs=10, verbose=1):\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n",
    "                      validation_split=0.05, verbose=verbose, shuffle=False)\n",
    "    #train_evaluate_showresults(history, model, model_name, \n",
    "    #                 X_train, Y_train, X_dev, Y_dev, X_test, Y_test,\n",
    "    #                 lag, batch_size, epochs, verbose)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_showresults(history, model, model_name, \n",
    "                   X_train, Y_train, Y_train_prevrawprice, X_dev, Y_dev, Y_dev_prevrawprice, X_test, Y_test, Y_test_prevrawprice,\n",
    "                   lag=10, batch_size=100, epochs=10, verbose=1):\n",
    "    # Plot losses, predictions, and calculate MSE and RMSE\n",
    "    plot_losses(history, 'Loss\\n(%s)' % model_name)\n",
    "    plot_predictions(model, X_dev, Y_dev, Y_dev_prevrawprice, 'Test Predictions\\n(%s)' % model_name)\n",
    "    plot_predictions(model, X_dev, Y_dev, Y_dev_prevrawprice, 'Test Predictions\\n(%s)' % model_name, inverse=True, scaler=price_scaler)\n",
    "    calculate_MSE_RMSE(model, price_scaler, X_dev, Y_dev, Y_dev_prevrawprice, '%s' % model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(model, model_name, \n",
    "                   X_train, Y_train, Y_train_prevrawprice, X_dev, Y_dev, Y_dev_prevrawprice, X_test, Y_test, Y_test_prevrawprice,\n",
    "                   lag=10, batch_size=100, epochs=10, verbose=1):\n",
    "    # Plot losses, predictions, and calculate MSE and RMSE\n",
    "    #plot_losses(history, 'Loss\\n(%s)' % model_name)\n",
    "    plot_predictions(model, X_test, Y_test, Y_test_prevrawprice, 'Test Predictions\\n(%s)' % model_name)\n",
    "    plot_predictions(model, X_test, Y_test, Y_test_prevrawprice, 'Test Predictions\\n(%s)' % model_name, inverse=True, scaler=price_scaler)\n",
    "    calculate_MSE_RMSE(model, price_scaler, X_test, Y_test, Y_test_prevrawprice, '%s' % model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(X_train, loss, optimizer, num_LSTMs, num_units, dropout):\n",
    "    \n",
    "    #LSTM_input_shape = [X_train.shape[1], X_train.shape[2]]\n",
    "    LSTM_input_shape = [X_train.shape[1], X_train.shape[2]]\n",
    "    print('input shape is')\n",
    "    print(LSTM_input_shape)\n",
    "\n",
    "    # DEFINE MODEL\n",
    "    model = Sequential()\n",
    "\n",
    "    if num_LSTMs == 2:\n",
    "            model.add(LSTM(num_units[0], input_shape=LSTM_input_shape, return_sequences=True))\n",
    "            model.add(Dropout(dropout))\n",
    "\n",
    "            model.add(LSTM(num_units[1], return_sequences=True))\n",
    "        \n",
    "    if num_LSTMs == 3:\n",
    "            model.add(LSTM(num_units[0], input_shape=LSTM_input_shape, return_sequences=True))\n",
    "            model.add(Dropout(dropout))\n",
    "\n",
    "            model.add(LSTM(num_units[1], return_sequences=True))\n",
    "            model.add(Dropout(dropout))\n",
    "            \n",
    "            model.add(LSTM(num_units[2], return_sequences=True))\n",
    "\n",
    "    #model.add(Dense(1))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    #model.add(Dense(1, kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    \n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    files = sorted(glob('cboe/parquet_preprocessed_BTCUSD/*.parquet'))\n",
    "    all_dataframes = []\n",
    "    for file in files:\n",
    "        print(file)\n",
    "        df = pq.read_table(file).to_pandas()\n",
    "        all_dataframes.append(df)\n",
    "    result = pd.concat(all_dataframes)\n",
    "    pq.write_table(pa.Table.from_pandas(result), 'cboe/parquet_preprocessed_subset_only_BTCUSD.parquet', compression='snappy')\n",
    "    df = pq.read_table('cboe/parquet_preprocessed_subset_only_BTCUSD.parquet').to_pandas();\n",
    "    print(df.dtypes)\n",
    "    print(df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_X(df):\n",
    "    n_all = df.shape[0]\n",
    "    n_train = round(n_all * 0.9)\n",
    "    n_dev   = round(n_all * 0.05)\n",
    "    n_test  = round(n_all * 0.05)\n",
    "    print('n_all:  ', n_all)\n",
    "    print('n_train:', n_train)\n",
    "    print('n_dev:  ', n_dev)\n",
    "    print('n_test: ', n_test)\n",
    "    \n",
    "#     X_train = df.iloc[:n_train, 1:-1].values.astype('float32')[:, None, :]\n",
    "#     X_dev   = df.iloc[n_train:n_train+n_dev, 1:-1].values.astype('float32')[:, None, :]\n",
    "#     X_test  = df.iloc[n_train+n_dev:, 1:-1].values.astype('float32')[:, None, :]\n",
    "#     X_train = df.iloc[:n_train, 1:-1].values.astype('float32')\n",
    "#     X_dev   = df.iloc[n_train:n_train+n_dev, 1:-1].values.astype('float32')\n",
    "#     X_test  = df.iloc[n_train+n_dev:, 1:-1].values.astype('float32')\n",
    "    X_train = df.iloc[:n_train, 1:16].values.astype('float32')\n",
    "    X_dev   = df.iloc[n_train:n_train+n_dev, 1:16].values.astype('float32')\n",
    "    X_test  = df.iloc[n_train+n_dev:, 1:16].values.astype('float32')\n",
    "    print(X_train.shape)\n",
    "    print(X_dev.shape)\n",
    "    print(X_test.shape)\n",
    "\n",
    "    return X_train, X_dev, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_Y(df):\n",
    "    n_all = df.shape[0]\n",
    "    n_train = round(n_all * 0.9)\n",
    "    n_dev   = round(n_all * 0.05)\n",
    "    n_test  = round(n_all * 0.05)\n",
    "    Y_train = df.iloc[:n_train, -1:].values.astype('float32')\n",
    "    Y_dev   = df.iloc[n_train:n_train+n_dev, -1:].values.astype('float32')\n",
    "    Y_test  = df.iloc[n_train+n_dev:, -1:].values.astype('float32')\n",
    "    print(Y_train.shape)\n",
    "    print(Y_dev.shape)\n",
    "    print(Y_test.shape)\n",
    "    \n",
    "    return Y_train, Y_dev, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_parquet(df, outfile):\n",
    "    pq.write_table(pa.Table.from_pandas(df), outfile, compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, history, X_train, X_dev, X_test, Y_train, Y_dev, Y_test):\n",
    "    train_loss = history.history['loss'][-1]\n",
    "    dev_loss = history.history['val_loss'][-1]\n",
    "    test_loss = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    \n",
    "    y_hat_train = model.predict(X_train)\n",
    "    y_hat_dev   = model.predict(X_dev)\n",
    "    y_hat_test  = model.predict(X_test)\n",
    "    \n",
    "    train_prop_correct = np.sum(np.sign(y_hat_test) == np.sign(Y_test)) / Y_test.shape[0]\n",
    "    dev_prop_correct   = np.sum(np.sign(y_hat_dev) == np.sign(Y_dev)) / Y_dev.shape[0]\n",
    "    test_prop_correct  = np.sum(np.sign(y_hat_train) == np.sign(Y_train)) / Y_train.shape[0]\n",
    "    \n",
    "    evaluation = [train_loss, dev_loss, test_loss, train_prop_correct, dev_prop_correct, test_prop_correct]\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = result\n",
    "# n_all = df.shape[0]\n",
    "# n_train = round(n_all * 0.9)\n",
    "# n_dev   = round(n_all * 0.05)\n",
    "# n_test  = round(n_all * 0.05)\n",
    "# print('n_all:  ', n_all)\n",
    "# print('n_train:', n_train)\n",
    "# print('n_dev:  ', n_dev)\n",
    "# print('n_test: ', n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequenced_data(data, window, step, y=True):\n",
    "    sequenced = []\n",
    "    for minute in range(0, len(data) - window, step):\n",
    "        chunk = data[minute:minute+window]\n",
    "        sequenced.append(chunk)\n",
    "    sequenced = np.array(sequenced)\n",
    "#     if y:\n",
    "#         sequenced = np.reshape(sequenced, [len(sequenced), window])  \n",
    "    return sequenced\n",
    "\n",
    "# def create_sequenced_data_Y(Y, window, step):\n",
    "#     sequenced = []\n",
    "#     for minute in range(0, len(data) - window, step):\n",
    "#         chunk = data[minute:minute+window]\n",
    "#         sequenced.append(chunk)\n",
    "#     Y_train_final = np.reshape(Y_train_final, [len(Y_train_final), 60]).shape\n",
    "#     return np.array(sequenced)\n",
    "\n",
    "\n",
    "\n",
    "# Y_train = df.iloc[:len(df), -1:].values.astype('float32')\n",
    "# Y_train_final = create_sequenced_data(Y_train, 60, 1)\n",
    "# Y_train_final = np.reshape(Y_train_final, [len(Y_train_final), 60]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(glob('cboe/parquet_preprocessed_BTCUSD_merged/*.parquet'))[800:801]\n",
    "all_dataframes = []\n",
    "for file in files:\n",
    "    df = pq.read_table(file).to_pandas()\n",
    "    all_dataframes.append(df)\n",
    "df = pd.concat(all_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_all:   1366\n",
      "n_train: 1229\n",
      "n_dev:   68\n",
      "n_test:  68\n",
      "(1229, 15)\n",
      "(68, 15)\n",
      "(69, 15)\n",
      "(1229, 1)\n",
      "(68, 1)\n",
      "(69, 1)\n",
      "Train, dev, test shapes:\n",
      "(1228, 1, 15)\n",
      "(67, 1, 15)\n",
      "(68, 1, 15)\n",
      "(1228, 1, 1)\n",
      "(67, 1, 1)\n",
      "(68, 1, 1)\n",
      "Train, dev, test shapes original:\n",
      "(1229, 15)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_dev, X_test = split_X(df)\n",
    "Y_train, Y_dev, Y_test = split_Y(df)\n",
    "\n",
    "window_size = 5\n",
    "\n",
    "X_train_final = create_sequenced_data(X_train, window=window_size, step=1, y=False)\n",
    "X_dev_final   = create_sequenced_data(X_dev, window=window_size, step=1, y=False)\n",
    "X_test_final  = create_sequenced_data(X_test, window=window_size, step=1, y=False)\n",
    "\n",
    "Y_train_final = create_sequenced_data(Y_train, window=window_size, step=1, y=True)\n",
    "Y_dev_final   = create_sequenced_data(Y_dev, window=window_size, step=1, y=True)\n",
    "Y_test_final  = create_sequenced_data(Y_test, window=window_size, step=1, y=True)\n",
    "\n",
    "print('Train, dev, test shapes:')\n",
    "print(X_train_final.shape)\n",
    "print(X_dev_final.shape)\n",
    "print(X_test_final.shape)\n",
    "print(Y_train_final.shape)\n",
    "print(Y_dev_final.shape)\n",
    "print(Y_test_final.shape)\n",
    "\n",
    "print('Train, dev, test shapes original:')\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape is\n",
      "[1, 15]\n",
      "Train on 1228 samples, validate on 67 samples\n",
      "Epoch 1/20\n",
      " - 10s - loss: 2.3920e-06 - val_loss: 1.2302e-06\n",
      "Epoch 2/20\n",
      " - 9s - loss: 1.8394e-06 - val_loss: 1.1802e-06\n",
      "Epoch 3/20\n",
      " - 9s - loss: 1.7645e-06 - val_loss: 1.1882e-06\n",
      "Epoch 4/20\n",
      " - 8s - loss: 1.7339e-06 - val_loss: 1.2070e-06\n",
      "Epoch 5/20\n",
      " - 9s - loss: 1.7227e-06 - val_loss: 1.1997e-06\n",
      "Epoch 6/20\n",
      " - 9s - loss: 1.7041e-06 - val_loss: 1.2075e-06\n",
      "Epoch 7/20\n",
      " - 9s - loss: 1.6913e-06 - val_loss: 1.2132e-06\n",
      "Epoch 8/20\n",
      " - 9s - loss: 1.6830e-06 - val_loss: 1.2153e-06\n",
      "Epoch 9/20\n",
      " - 9s - loss: 1.6691e-06 - val_loss: 1.2198e-06\n",
      "Epoch 10/20\n",
      " - 8s - loss: 1.6730e-06 - val_loss: 1.2251e-06\n",
      "Epoch 11/20\n",
      " - 9s - loss: 1.6721e-06 - val_loss: 1.2243e-06\n",
      "Epoch 12/20\n",
      " - 8s - loss: 1.6675e-06 - val_loss: 1.2277e-06\n",
      "Epoch 13/20\n",
      " - 8s - loss: 1.6667e-06 - val_loss: 1.2307e-06\n",
      "Epoch 14/20\n",
      " - 8s - loss: 1.6587e-06 - val_loss: 1.2306e-06\n",
      "Epoch 15/20\n",
      " - 9s - loss: 1.6579e-06 - val_loss: 1.2335e-06\n",
      "Epoch 16/20\n",
      " - 8s - loss: 1.6541e-06 - val_loss: 1.2334e-06\n",
      "Epoch 17/20\n",
      " - 9s - loss: 1.6562e-06 - val_loss: 1.2355e-06\n",
      "Epoch 18/20\n",
      " - 9s - loss: 1.6504e-06 - val_loss: 1.2360e-06\n",
      "Epoch 19/20\n",
      " - 8s - loss: 1.6493e-06 - val_loss: 1.2382e-06\n",
      "Epoch 20/20\n",
      " - 8s - loss: 1.6482e-06 - val_loss: 1.2392e-06\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "epochs = 20\n",
    "verbose = 2\n",
    "loss = 'mean_squared_error'\n",
    "optimizer = 'adagrad'\n",
    "num_LSTM = 2\n",
    "#n_units = [256, 256]\n",
    "n_units = [1, 1]\n",
    "dropout = 0.1\n",
    "\n",
    "model = initialize_model(X_train_final, loss, optimizer, num_LSTM, n_units, dropout)\n",
    "\n",
    "history = model.fit(X_train_final, Y_train_final, batch_size=batch_size, epochs=epochs,\n",
    "                      validation_data=(X_dev_final, Y_dev_final), verbose=verbose, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "loss = 'mse'\n",
    "optimizers = ['adagrad', 'adam', 'rmsprop']\n",
    "batch_sizes = [128, 8192, 16384]\n",
    "include_monthDayHour = [True, False]\n",
    "num_LSTMs = [2,3]\n",
    "num_units_2 = [[128, 256], [256, 256]]\n",
    "num_units_3 = [[128, 256, 256], [256, 256, 256], [256, 512, 512]]\n",
    "#amount_of_data = {'one_year', 'all'}\n",
    "#dropout = np.random.uniform(0.5, 0.05, num_LSTMs)\n",
    "dropout = 0.1\n",
    "\n",
    "# Load data\n",
    "df = load_data()\n",
    "X_train, X_dev, X_test = split_X(df) \n",
    "Y_train, Y_dev, Y_test = split_Y(df)\n",
    "\n",
    "df_noTime = df.iloc[:,1:16]\n",
    "X_train_noTime, X_dev_noTime, X_test_noTime = split_X(df_noTime) \n",
    "\n",
    "# Initialize output dataframe\n",
    "outfile = 'cboe/grid_search_dataSubset.parquet'\n",
    "columns = ['num_epochs', 'loss', 'optimizer', 'batch_size', 'include_time', 'num_LSTMs', 'num_units',\n",
    "           'train_loss', 'dev_loss', 'test_loss', 'train_prop_correct', 'dev_prop_correct', 'test_prop_correct']\n",
    "df_output = pd.DataFrame(columns=columns)\n",
    "pq.write_table(pa.Table.from_pandas(df_output), outfile, compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for include_time in include_monthDayHour:\n",
    "    if not include_time:\n",
    "        X_train = X_train_noTime\n",
    "        X_dev =   X_dev_noTime\n",
    "        X_test =  X_test_noTime\n",
    "\n",
    "    for optimizer in optimizers:\n",
    "        for batch_size in batch_sizes:\n",
    "            for num_LSTM in num_LSTMs:\n",
    "                if num_LSTM == 2:\n",
    "                    num_units = num_units_2\n",
    "                elif num_LSTM == 3:\n",
    "                    num_units = num_units_3\n",
    "                for n_units in num_units:\n",
    "                    # Load output dataframe\n",
    "                    df_output = pq.read_table(outfile).to_pandas()\n",
    "                    \n",
    "                    # Initialize model\n",
    "                    model = initialize_model(X_train, loss, optimizer, num_LSTM, n_units, dropout)\n",
    "                    \n",
    "                    # Train model\n",
    "                    if count%10==0:\n",
    "                        verbose=1\n",
    "                        print(count, '/', 120)\n",
    "                    else:\n",
    "                        verbose=0\n",
    "                    history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n",
    "                      validation_data=(X_dev, Y_dev), verbose=verbose, shuffle=False) \n",
    "                    \n",
    "                    # Evaluate model\n",
    "                    evaluate = evaluate_model(model, history, X_train, X_dev, X_test, Y_train, Y_dev, Y_test)\n",
    "                    \n",
    "                    # Write to dataframe and save\n",
    "                    row = [num_epochs, loss, optimizer, batch_size, include_time, num_LSTM, str(n_units)]\n",
    "                    row.extend(evaluate)\n",
    "                    df_output.loc[len(df_output)] = row\n",
    "                    df_to_parquet(df_output, outfile)\n",
    "                    \n",
    "                    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2048 batch, 3 cells, numunits[2]\n",
    "# HYPERPARAMETERS\n",
    "#####################\n",
    "loss = 'mean_squared_error'\n",
    "optimizer = 'adagrad'\n",
    "batch_size = 2048\n",
    "epochs = 10\n",
    "\n",
    "num_LSTMs = 3\n",
    "num_units = num_units_3[2]\n",
    "model = initialize_model(X_train, loss, optimizer, num_LSTMs, num_units, dropout)\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n",
    "                      validation_data=(X_dev, Y_dev), verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
