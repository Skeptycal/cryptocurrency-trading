{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "import lz4.frame\n",
    "import gzip\n",
    "import io\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from plumbum.cmd import rm\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import pickle\n",
    "import datetime\n",
    "import re\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotline(data):\n",
    "    plt.figure()\n",
    "    plt.plot(data)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def event_count(time_series, data_name):\n",
    "    time_series = time_series[['Fill Price (USD)']].values\n",
    "    upevents = 0\n",
    "    downevents = 0\n",
    "    sameprice = 0\n",
    "    prev_obv = time_series[0]\n",
    "    for obv in time_series[1:]:\n",
    "        if obv > prev_obv:\n",
    "            upevents += 1\n",
    "        elif obv < prev_obv:\n",
    "            downevents += 1\n",
    "        elif obv == prev_obv:\n",
    "            sameprice += 1\n",
    "        prev_obv = obv\n",
    "    print('=== Event counts on %s ===' % data_name)\n",
    "    print('upevents')\n",
    "    print(upevents)\n",
    "    print('downevents')\n",
    "    print(downevents)\n",
    "    print('sameprice')\n",
    "    print(sameprice)\n",
    "    print()\n",
    "\n",
    "def mse(time_series, data_name):\n",
    "    time_series = time_series[['Fill Price (USD)']].values\n",
    "    total_squared_error = 0\n",
    "    total_absolute_error = 0\n",
    "    prev_obv = time_series[0]\n",
    "    for obv in time_series[1:]:\n",
    "        total_squared_error += (obv - prev_obv)**2\n",
    "        total_absolute_error += abs(obv - prev_obv)\n",
    "        prev_obv = obv\n",
    "    num_predictions = len(time_series) - 1\n",
    "    mean_squared_error = total_squared_error / num_predictions\n",
    "    mean_absolute_error = total_absolute_error / num_predictions\n",
    "    root_mean_squared_error = np.sqrt(mean_squared_error)\n",
    "    print('=== baseline on %s ===' % data_name)\n",
    "    print('total squared error')\n",
    "    print(total_squared_error)\n",
    "    print('total absolute error')\n",
    "    print(total_absolute_error)\n",
    "    print('mean squared error')\n",
    "    print(mean_squared_error)\n",
    "    print('mean absolute error')\n",
    "    print(mean_absolute_error) \n",
    "    print('root mean squared error')\n",
    "    print(root_mean_squared_error) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_summary_statistics():\n",
    "    #event_count(small_set, 'small')\n",
    "    train_set = df.iloc[0:num_samples_training]\n",
    "    dev_set = df.iloc[num_samples_training:num_samples_training+num_samples_dev]\n",
    "    test_set = df.iloc[num_samples_training+num_samples_dev:]\n",
    "    event_count(train_set, 'train')\n",
    "    event_count(dev_set, 'dev')\n",
    "    event_count(test_set, 'test')\n",
    "    mse(train_set, 'train')\n",
    "    mse(dev_set, 'dev')\n",
    "    mse(test_set, 'test')\n",
    "#show_summary_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    values = np.array(data)\n",
    "    values = values.reshape(-1,1)\n",
    "    values = values.astype('float32') \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(model_history, title):\n",
    "    plt.figure()\n",
    "    plt.plot(model_history.history['loss'], label='Train')\n",
    "    plt.plot(model_history.history['val_loss'], label='Dev')\n",
    "    plt.xlabel('Epochs'); plt.ylabel('Loss (mse)')\n",
    "    plt.title(title)\n",
    "    plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform_pricescaler(data, Y_prevrawprice, fitted_scaler):\n",
    "    return fitted_scaler.inverse_transform(preprocess(data))\n",
    "\n",
    "def inverse_transform_percentdiff(data, Y_prevrawprice, fitted_scaler=None):\n",
    "    orig_prices = Y_prevrawprice\n",
    "    change = orig_prices * data\n",
    "    return orig_prices + change\n",
    "    #return fitted_scaler.inverse_transform(preprocess(data))\n",
    "\n",
    "#print(Y_test_prevrawprice)\n",
    "#print(inverse_transform_percentdiff(Y_test, Y_test_prevrawprice))\n",
    "\n",
    "inverse_transform = inverse_transform_percentdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(model, X_test, Y_test, Y_prevrawprice, title, inverse=False, scaler=None):\n",
    "    y_hat = model.predict(X_test)\n",
    "\n",
    "    if inverse:\n",
    "        y_hat = inverse_transform(y_hat, Y_prevrawprice, scaler)\n",
    "        Y_test = inverse_transform(Y_test, Y_prevrawprice, scaler)\n",
    "\n",
    "    plt.plot(y_hat, label='Predicted')\n",
    "    plt.plot(Y_test, label='True')\n",
    "    plt.xlabel('Time'); \n",
    "\n",
    "    if inverse:\n",
    "        plt.ylabel('Price')\n",
    "    else:\n",
    "        plt.ylabel('RESCALED Price')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MSE_RMSE(model, scaler, X_test, Y_test, Y_prevrawprice, model_name):\n",
    "    y_hat = model.predict(X_test)\n",
    "    y_hat_inverse = inverse_transform(y_hat, Y_prevrawprice, scaler)\n",
    "    Y_test_inverse = inverse_transform(Y_test, Y_prevrawprice, scaler)\n",
    "    mse = mean_squared_error(Y_test_inverse, y_hat_inverse)\n",
    "    rmse = np.sqrt(mean_squared_error(Y_test_inverse, y_hat_inverse))\n",
    "    print('%s:' % model_name)\n",
    "    print('Test MSE: %.3f' % mse)\n",
    "    print('Test RMSE: %.3f' % rmse)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(model, model_name, \n",
    "                   X_train, Y_train, Y_train_prevrawprice, X_dev, Y_dev, Y_dev_prevrawprice, X_test, Y_test, Y_test_prevrawprice,\n",
    "                   lag=10, batch_size=100, epochs=10, verbose=1):\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n",
    "                      validation_split=0.05, verbose=verbose, shuffle=False)\n",
    "    #train_evaluate_showresults(history, model, model_name, \n",
    "    #                 X_train, Y_train, X_dev, Y_dev, X_test, Y_test,\n",
    "    #                 lag, batch_size, epochs, verbose)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_showresults(history, model, model_name, \n",
    "                   X_train, Y_train, Y_train_prevrawprice, X_dev, Y_dev, Y_dev_prevrawprice, X_test, Y_test, Y_test_prevrawprice,\n",
    "                   lag=10, batch_size=100, epochs=10, verbose=1):\n",
    "    # Plot losses, predictions, and calculate MSE and RMSE\n",
    "    plot_losses(history, 'Loss\\n(%s)' % model_name)\n",
    "    plot_predictions(model, X_dev, Y_dev, Y_dev_prevrawprice, 'Test Predictions\\n(%s)' % model_name)\n",
    "    plot_predictions(model, X_dev, Y_dev, Y_dev_prevrawprice, 'Test Predictions\\n(%s)' % model_name, inverse=True, scaler=price_scaler)\n",
    "    calculate_MSE_RMSE(model, price_scaler, X_dev, Y_dev, Y_dev_prevrawprice, '%s' % model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(model, model_name, \n",
    "                   X_train, Y_train, Y_train_prevrawprice, X_dev, Y_dev, Y_dev_prevrawprice, X_test, Y_test, Y_test_prevrawprice,\n",
    "                   lag=10, batch_size=100, epochs=10, verbose=1):\n",
    "    # Plot losses, predictions, and calculate MSE and RMSE\n",
    "    #plot_losses(history, 'Loss\\n(%s)' % model_name)\n",
    "    plot_predictions(model, X_test, Y_test, Y_test_prevrawprice, 'Test Predictions\\n(%s)' % model_name)\n",
    "    plot_predictions(model, X_test, Y_test, Y_test_prevrawprice, 'Test Predictions\\n(%s)' % model_name, inverse=True, scaler=price_scaler)\n",
    "    calculate_MSE_RMSE(model, price_scaler, X_test, Y_test, Y_test_prevrawprice, '%s' % model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(X_train, loss, optimizer, num_LSTMs, num_units, dropout, predict_end_of_window):\n",
    "    \n",
    "    LSTM_input_shape = [X_train.shape[1], X_train.shape[2]]\n",
    "\n",
    "    # DEFINE MODEL\n",
    "    model = Sequential()\n",
    "\n",
    "    if num_LSTMs == 2:\n",
    "            model.add(LSTM(num_units[0], input_shape=LSTM_input_shape, return_sequences=True))\n",
    "            model.add(Dropout(dropout))\n",
    "            \n",
    "            if predict_end_of_window:\n",
    "                model.add(LSTM(num_units[1], return_sequences=False))\n",
    "            else:\n",
    "                model.add(LSTM(num_units[1], return_sequences=True))\n",
    "        \n",
    "    if num_LSTMs == 3:\n",
    "            model.add(LSTM(num_units[0], input_shape=LSTM_input_shape, return_sequences=True))\n",
    "            model.add(Dropout(dropout))\n",
    "\n",
    "            model.add(LSTM(num_units[1], return_sequences=True))\n",
    "            model.add(Dropout(dropout))\n",
    "            \n",
    "            if predict_end_of_window:\n",
    "                model.add(LSTM(num_units[2], return_sequences=False))\n",
    "            else:\n",
    "                model.add(LSTM(num_units[2], return_sequences=True))\n",
    "\n",
    "    if predict_end_of_window:\n",
    "        model.add(Dense(1))\n",
    "    else:\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "      \n",
    "    model.add(Activation('linear'))\n",
    "    \n",
    "    \n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os.path\n",
    "\n",
    "# def load_data():\n",
    "#     if not os.path.isfile('cboe/parquet_preprocessed_subset_only_BTCUSD_merged.parquet'):\n",
    "#         files = sorted(glob('cboe/parquet_preprocessed_BTCUSD_merged/*.parquet'))\n",
    "#         all_dataframes = []\n",
    "#         for file in files:\n",
    "#             print(file)\n",
    "#             df = pq.read_table(file).to_pandas()\n",
    "#             all_dataframes.append(df)\n",
    "#         result = pd.concat(all_dataframes)\n",
    "#         pq.write_table(pa.Table.from_pandas(result), 'cboe/parquet_preprocessed_subset_only_BTCUSD_merged.parquet', compression='snappy')\n",
    "#     df = pq.read_table('cboe/parquet_preprocessed_subset_only_BTCUSD_merged.parquet').to_pandas();\n",
    "#     print(df.dtypes)\n",
    "#     print(df.shape)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_X(df, temporal_features, percent_train):\n",
    "    n_all = df.shape[0]\n",
    "    n_train = round(n_all * percent_train)\n",
    "    n_dev   = round(n_all * ((1 - percent_train)/2))\n",
    "    n_test  = round(n_all * ((1 - percent_train)/2))\n",
    "    print('n_all:  ', n_all)\n",
    "    print('n_train:', n_train)\n",
    "    print('n_dev:  ', n_dev)\n",
    "    print('n_test: ', n_test)\n",
    "    \n",
    "    if temporal_features:\n",
    "        end = 59\n",
    "    else:\n",
    "        end = 16\n",
    "\n",
    "    X_train = df.iloc[:n_train, 1:end].values.astype('float32')\n",
    "    X_dev   = df.iloc[n_train:n_train+n_dev, 1:end].values.astype('float32')\n",
    "    X_test  = df.iloc[n_train+n_dev:, 1:end].values.astype('float32')\n",
    "    print(X_train.shape)\n",
    "    print(X_dev.shape)\n",
    "    print(X_test.shape)\n",
    "\n",
    "    return X_train, X_dev, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_Y(df, percent_train):\n",
    "    n_all = df.shape[0]\n",
    "    n_train = round(n_all * percent_train)\n",
    "    n_dev   = round(n_all * ((1 - percent_train)/2))\n",
    "    n_test  = round(n_all * ((1 - percent_train)/2))\n",
    "    Y_train = df.iloc[:n_train, -1:].values.astype('float32')\n",
    "    Y_dev   = df.iloc[n_train:n_train+n_dev, -1:].values.astype('float32')\n",
    "    Y_test  = df.iloc[n_train+n_dev:, -1:].values.astype('float32')\n",
    "    print(Y_train.shape)\n",
    "    print(Y_dev.shape)\n",
    "    print(Y_test.shape)\n",
    "    \n",
    "    return Y_train, Y_dev, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_parquet(df, outfile):\n",
    "    pq.write_table(pa.Table.from_pandas(df), outfile, compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direction_prediction(y_true, y_pred, predict_end_of_window):\n",
    "    if predict_end_of_window:\n",
    "        prop_correct = np.sum(np.sign(y_pred) == np.sign(y_true)) / y_true.shape[0]\n",
    "    else:\n",
    "        prop_correct = np.sum(np.sign(y_pred) == np.sign(y_true)) / (y_true.shape[0] * y_true.shape[1])\n",
    "    return prop_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, history, X_train, X_dev, X_test, Y_train, Y_dev, Y_test, predict_end_of_window):\n",
    "    train_loss = history.history['loss'][-1]\n",
    "    dev_loss   = history.history['val_loss'][-1]\n",
    "    \n",
    "    print('Evaluating test loss...')\n",
    "    test_loss  = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    \n",
    "    print('Predicting y_hat_train...')\n",
    "    y_hat_train = model.predict(X_train)\n",
    "    print('Predicting y_hat_dev...')\n",
    "    y_hat_dev   = model.predict(X_dev)\n",
    "    print('Predicting y_hat_test...')\n",
    "    y_hat_test  = model.predict(X_test)\n",
    "    \n",
    "    train_prop_correct = direction_prediction(Y_train, y_hat_train, predict_end_of_window)\n",
    "    dev_prop_correct   = direction_prediction(Y_dev, y_hat_dev, predict_end_of_window)\n",
    "    test_prop_correct  = direction_prediction(Y_test, y_hat_test, predict_end_of_window)\n",
    "    \n",
    "    evaluation = {'train_loss': train_loss,\n",
    "                  'dev_loss': dev_loss,\n",
    "                  'test_loss': test_loss,\n",
    "                  'train_prop_correct': train_prop_correct,\n",
    "                  'dev_prop_correct': dev_prop_correct,\n",
    "                  'test_prop_correct': test_prop_correct,\n",
    "                  'y_hat_train': y_hat_train,\n",
    "                  'y_hat_dev': y_hat_dev,\n",
    "                  'y_hat_test': y_hat_test}\n",
    "\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequenced_data(data, window, step):\n",
    "    sequenced = []\n",
    "    for minute in range(0, len(data) - window + 1, step):\n",
    "        chunk = data[minute:minute+window]\n",
    "        sequenced.append(chunk)\n",
    "    sequenced = np.array(sequenced)\n",
    "    return sequenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df, temporal_features, percent_train):\n",
    "    X_train, X_dev, X_test = split_X(df, temporal_features, percent_train)\n",
    "    Y_train, Y_dev, Y_test = split_Y(df, percent_train)\n",
    "    return X_train, X_dev, X_test, Y_train, Y_dev, Y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, day_start=None, day_end=None):\n",
    "\n",
    "    # Concatenate dataframes\n",
    "    files = sorted(glob('%s/*.parquet' % path))\n",
    "    \n",
    "    if day_start is not None:\n",
    "        start = day_start\n",
    "    else:\n",
    "        start = 0\n",
    "    if day_end is not None:\n",
    "        end = day_start\n",
    "    else:\n",
    "        end = len(files)\n",
    "    files = files[start:end]\n",
    "    \n",
    "    all_dataframes = []\n",
    "    for file in files:\n",
    "        df = pq.read_table(file).to_pandas()\n",
    "        all_dataframes.append(df)\n",
    "    df = pd.concat(all_dataframes)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_end_of_window_Y(Y, window, step):\n",
    "    return np.array([Y[i] for i in range(window-1, len(Y), step)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_XY(X_train, X_dev, X_test, Y_train, Y_dev, Y_test, window_size, step, predict_end_of_window):\n",
    "    X_train = create_sequenced_data(X_train, window=window_size, step=step)\n",
    "    X_dev   = create_sequenced_data(X_dev,   window=window_size, step=step)\n",
    "    X_test  = create_sequenced_data(X_test,  window=window_size, step=step)\n",
    "\n",
    "    if predict_end_of_window:\n",
    "        Y_train = create_end_of_window_Y(Y_train, window=window_size, step=step)\n",
    "        Y_dev   = create_end_of_window_Y(Y_dev,   window=window_size, step=step)\n",
    "        Y_test  = create_end_of_window_Y(Y_test,  window=window_size, step=step)\n",
    "    else:\n",
    "        Y_train = create_sequenced_data(Y_train, window=window_size, step=step)\n",
    "        Y_dev   = create_sequenced_data(Y_dev,   window=window_size, step=step)\n",
    "        Y_test  = create_sequenced_data(Y_test,  window=window_size, step=step)\n",
    "    \n",
    "    print('Train, dev, test shapes:')\n",
    "    print(X_train.shape)\n",
    "    print(X_dev.shape)\n",
    "    print(X_test.shape)\n",
    "    print(Y_train.shape)\n",
    "    print(Y_dev.shape)\n",
    "    print(Y_test.shape)\n",
    "    \n",
    "    return X_train, X_dev, X_test, Y_train, Y_dev, Y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_history(model, history, model_path):\n",
    "    # serialize model to JSON\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        suffix = ''.join(re.findall(r'\\d+', str(datetime.datetime.now())))\n",
    "        model_path = model_path + '_' + suffix\n",
    "\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "    model_json = model.to_json()   \n",
    "    with open(model_path + '/model.json', 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(model_path + '/model.h5')\n",
    "\n",
    "    print(\"Saved model and history to:\\n%s\" % model_path)\n",
    "\n",
    "    with open(model_path + '/trainHistoryDict', 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_price(df, X_train, X_dev, field):\n",
    "    X_train_stop = len(X_train)\n",
    "    X_dev_stop = X_train_stop + len(X_dev)\n",
    "\n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.plot(np.arange(0, X_train_stop), df.iloc[0:X_train_stop][field], 'k')\n",
    "    plt.plot(np.arange(X_train_stop, X_dev_stop), df.iloc[X_train_stop:X_dev_stop][field], 'r')\n",
    "    plt.plot(np.arange(X_dev_stop, len(df)), df.iloc[X_dev_stop:len(df)][field], 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_dev_losses(history):\n",
    "    train_loss = history.history['loss']\n",
    "    dev_loss   = history.history['val_loss']\n",
    "    \n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.plot(np.log(train_loss), 'k')\n",
    "    \n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.plot(np.log(dev_loss), 'b')\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.plot(train_loss, 'k')\n",
    "    \n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.plot(dev_loss, 'b')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_percent_change(y_pred, y_true, timestep_within_window, minute_start, minute_end, predict_end_of_window):\n",
    "    \n",
    "    \n",
    "    ys=[]\n",
    "    for i in range(len(y_pred)):\n",
    "        ys.append(y_pred[i][timestep_within_window])\n",
    "\n",
    "    original_ys=[]\n",
    "    for i in range(len(y_true)):\n",
    "        original_ys.append(y_true[i][timestep_within_window])\n",
    "\n",
    "        \n",
    "    ys_orig = np.array(original_ys)\n",
    "    ys_pred = np.array(ys)\n",
    "    \n",
    "    \n",
    "    OldRange = (ys_pred.max() - ys_pred.min())  \n",
    "    NewRange = (ys_orig.max() - ys_orig.min())   \n",
    "    new_ys_pred = (((ys - ys_pred.min()) * NewRange) / OldRange) + ys_orig.min()\n",
    "    \n",
    "    \n",
    "    norm1 = ys_orig / np.linalg.norm(ys_orig)\n",
    "    norm2 = ys_pred / np.linalg.norm(ys_pred)\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(norm1[minute_start:minute_end], 'k', alpha=0.9)\n",
    "    plt.plot(norm2[minute_start:minute_end], 'r', alpha=0.9)\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(original_ys[minute_start:minute_end], 'k', alpha=0.9)\n",
    "    plt.plot(ys[minute_start:minute_end], 'r', alpha=0.9)\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(original_ys[minute_start:minute_end], 'k', alpha=0.9)\n",
    "    plt.plot(new_ys_pred[minute_start:minute_end], 'r', alpha=0.9)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.plot(y_true[minute_start:minute_end], 'k', alpha=0.9)\n",
    "        plt.plot(y_pred[minute_start:minute_end], 'r', alpha=0.9)\n",
    "    except:\n",
    "        a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_save_events_props(train, dev, test, evaluate, model_name, model_path):\n",
    "    \n",
    "    train_event_counts = np.unique(np.sign(train), return_counts=True)\n",
    "    train_event_prop   = train_event_counts[1] / len(train)\n",
    "    \n",
    "    dev_event_counts = np.unique(np.sign(dev), return_counts=True)\n",
    "    dev_event_prop   = dev_event_counts[1] / len(dev)\n",
    "    \n",
    "    test_event_counts = np.unique(np.sign(test), return_counts=True)\n",
    "    test_event_prop   = test_event_counts[1] / len(test)\n",
    "    \n",
    "    print(model_name)\n",
    "    print('\\n========== EVENT COUNTS AND PROPORTIONS ==========')\n",
    "    print('=== TRAIN ===')\n",
    "    print('Down, Same, Up:', train_event_counts[1])\n",
    "    print('Down, Same, Up:', train_event_prop)\n",
    "    \n",
    "    print('\\n=== DEV ===')\n",
    "    print('Down, Same, Up:', dev_event_counts[1])\n",
    "    print('Down, Same, Up:', dev_event_prop)\n",
    "    \n",
    "    print('\\n=== TEST ===')\n",
    "    print('Down, Same, Up:', test_event_counts[1])\n",
    "    print('Down, Same, Up:', test_event_prop)\n",
    "    \n",
    "    \n",
    "    print('\\n========== CORRECTION DIRECTION PREDICTIONS ==========')\n",
    "    print(\"TRAIN: %f\\nDEV:   %f\\nTEST:  %f\" % (evaluate['train_prop_correct'], \n",
    "                                               evaluate['dev_prop_correct'], \n",
    "                                               evaluate['test_prop_correct']))\n",
    "    \n",
    "    print('\\n========== FINAL LOSS ==========')\n",
    "    print(\"TRAIN: %s\\nDEV:   %s\\nTEST:  %s\\n\" % (evaluate['train_loss'], evaluate['dev_loss'], evaluate['test_loss']))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO RESTORE MODEL:\n",
    "# # load json and create model\n",
    "# json_file = open('model.json', 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "# # load weights into new model\n",
    "# loaded_model.load_weights(\"model.h5\")\n",
    "# print(\"Loaded model from disk\")\n",
    " \n",
    "# # evaluate loaded model on test data\n",
    "# loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "# score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "# print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_end_of_window' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e1a58c349770>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mnum_units_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnum_units\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m model_name = 'window-%s_step-%s_predEndWindow-%s_batch-%s_epochs-%s_loss-%s_opt-%s_numLSTMs-%s_numUnits-%s_dropout-%s_dayStart-%s_dayEnd-%s' % (window_size, step, str(predict_end_of_window), batch_size, num_epochs, \n\u001b[0m\u001b[1;32m     24\u001b[0m                                                                                                                                                 loss, optimizer, num_LSTM, num_units_string, dropout, str(day_start), str(day_end))\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_end_of_window' is not defined"
     ]
    }
   ],
   "source": [
    "##### MAIN MODEL #####\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "window_size = 60\n",
    "step = 1\n",
    "\n",
    "batch_size = 8192\n",
    "num_epochs = 50\n",
    "verbose = 1\n",
    "loss = 'mean_squared_error'\n",
    "optimizer = 'adam'\n",
    "# num_LSTM = 3\n",
    "# num_units = [128, 256, 256]\n",
    "num_LSTM = 2\n",
    "num_units = [256, 256]\n",
    "dropout = 0.1\n",
    "\n",
    "path = 'cboe/parquet_preprocessed_BTCUSD_merged'\n",
    "day_start = 401\n",
    "day_end = None\n",
    "num_units_string = '-'.join([str(u) for u in num_units])\n",
    "\n",
    "model_name = 'window-%s_step-%s_predEndWindow-%s_batch-%s_epochs-%s_loss-%s_opt-%s_numLSTMs-%s_numUnits-%s_dropout-%s_dayStart-%s_dayEnd-%s' % (window_size, step, str(predict_end_of_window), batch_size, num_epochs, \n",
    "                                                                                                                                                loss, optimizer, num_LSTM, num_units_string, dropout, str(day_start), str(day_end))\n",
    "print(model_name+'\\n')\n",
    "model_path = 'models/%s' % model_name\n",
    "\n",
    "# LOAD DATA\n",
    "df = load_data(path, day_start, day_end)\n",
    "\n",
    "# CREATE XY DATA\n",
    "X_train, X_dev, X_test, Y_train, Y_dev, Y_test = split(df)\n",
    "X_train, X_dev, X_test, Y_train, Y_dev, Y_test = create_all_XY(X_train, X_dev, X_test, \n",
    "                                                               Y_train, Y_dev, Y_test,\n",
    "                                                               window_size, step)\n",
    "# INITIALIZE MODEL\n",
    "model = initialize_model(X_train, loss, optimizer, num_LSTM, num_units, dropout)\n",
    "\n",
    "# TRAIN MODEL\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "                      validation_data=(X_dev, Y_dev), verbose=verbose, shuffle=False) \n",
    "\n",
    "# SAVE MODEL AND HISTORY\n",
    "save_model_history(model, history, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "model = model_from_json(open(model_path + '/model.json').read())\n",
    "model.load_weights(model_path + '/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected lstm_1_input to have 3 dimensions, but got array with shape (29047, 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-0d9c4441ba45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_hat_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[0;32m-> 1025\u001b[0;31m                                   steps=steps)\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1822\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1823\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1824\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1825\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1826\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    111\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected lstm_1_input to have 3 dimensions, but got array with shape (29047, 15)"
     ]
    }
   ],
   "source": [
    "#y_hat_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-1b0087c3c18a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Evaluating...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mevaluate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Get test predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Getting test predictions...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "print('Evaluating...')\n",
    "evaluate = evaluate_model(model, history, X_train, X_dev, X_test, Y_train, Y_dev, Y_test)\n",
    "\n",
    "# Get test predictions\n",
    "print('Getting test predictions...')\n",
    "y_hat_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-0d9c4441ba45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_hat_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL AND HISTORY\n",
    "#save_model_history(model, history, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# EVALUATE MODEL\n",
    "print('Evaluating...')\n",
    "evaluate = evaluate_model(model, history, X_train, X_dev, X_test, Y_train, Y_dev, Y_test)\n",
    "\n",
    "# Get test predictions\n",
    "print('Getting test predictions...')\n",
    "y_hat_test = model.predict(X_test)\n",
    "\n",
    "# VISUALIZE\n",
    "## Plot: \n",
    "# Historical price, color coded with train, dev, test\n",
    "# Historical percent change, color coded with train, dev, test\n",
    "# Train Loss, Dev Loss\n",
    "# Actual price vs predicted price (or percent change) for test set\n",
    "# Example features time series for one day (NOTE: in the preprocessing_final notebook)\n",
    "\n",
    "timestep_within_window = 0\n",
    "minute_start = 59\n",
    "minute_end = len(Y_test)\n",
    "\n",
    "print_save_events_props(Y_train.flatten(), Y_dev.flatten(), Y_test.flatten(), evaluate, model_name, model_path)\n",
    "#print_save_events_props(Y_train, Y_dev, Y_test, evaluate, model_name, model_path)\n",
    "plot_price(df, X_train, X_dev, field='current_price')\n",
    "plot_price(df, X_train, X_dev, field='percent_change')\n",
    "plot_train_dev_losses(history)\n",
    "plot_percent_change(y_hat_test, Y_test, timestep_within_window, minute_start, minute_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = y_hat_test\n",
    "# y_true = Y_test\n",
    "# timestep_within_window=59\n",
    "# ys=[]\n",
    "# for i in range(len(y_pred)):\n",
    "#     ys.append(y_pred[i][timestep_within_window])\n",
    "\n",
    "# original_ys=[]\n",
    "# for i in range(len(y_true)):\n",
    "#     original_ys.append(y_true[i][timestep_within_window])\n",
    "\n",
    "# ys_orig = np.array(original_ys)\n",
    "# ys_pred = np.array(ys)\n",
    "\n",
    "\n",
    "# OldRange = (ys_pred.max() - ys_pred.min())  \n",
    "# NewRange = (ys_orig.max() - ys_orig.min())   \n",
    "# new_ys_pred = (((ys - ys_pred.min()) * NewRange) / OldRange) + ys_orig.min()\n",
    "\n",
    "\n",
    "# norm1 = ys_orig / np.linalg.norm(ys_orig)\n",
    "# norm2 = ys_pred / np.linalg.norm(ys_pred)\n",
    "\n",
    "# mse_test_norm = sum((norm2-norm1)**2)\n",
    "# print(mse_test_norm)\n",
    "# print(np.sqrt(mse_test_norm/len(norm2)))\n",
    "\n",
    "# mse_test = sum((ys_pred-ys_orig)**2)\n",
    "# print(mse_test)\n",
    "# print(np.sqrt(mse_test/len(norm2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(time_series):\n",
    "    total_squared_error = 0\n",
    "    total_absolute_error = 0\n",
    "    prev_obv = time_series[0]\n",
    "    for obv in time_series[1:]:\n",
    "        total_squared_error += (obv - prev_obv)**2\n",
    "        total_absolute_error += abs(obv - prev_obv)\n",
    "        prev_obv = obv\n",
    "    num_predictions = len(time_series) - 1\n",
    "    mean_squared_error = total_squared_error / num_predictions\n",
    "    mean_absolute_error = total_absolute_error / num_predictions\n",
    "    root_mean_squared_error = np.sqrt(mean_squared_error)\n",
    "    print('=== baseline ===')\n",
    "    print('total squared error')\n",
    "    print(total_squared_error)\n",
    "    print('total absolute error')\n",
    "    print(total_absolute_error)\n",
    "    print('mean squared error')\n",
    "    print(mean_squared_error)\n",
    "    print('mean absolute error')\n",
    "    print(mean_absolute_error) \n",
    "    print('root mean squared error')\n",
    "    print(root_mean_squared_error) \n",
    "    print()\n",
    "mse(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window-60_step-1_predEndWindow-True_batch-4096_epochs-30_loss-mean_squared_error_opt-adam_numLSTMs-3_numUnits-[128, 256, 256]_dropout-0.1_dayStart-401_dayEnd-None\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "split() missing 2 required positional arguments: 'temporal_features' and 'percent_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-49de82fe86d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# CREATE XY DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m X_train, X_dev, X_test, Y_train, Y_dev, Y_test = create_all_XY(X_train, X_dev, X_test, Y_train, Y_dev, Y_test,\n\u001b[1;32m     34\u001b[0m                                                                window_size, step, predict_end_of_window)\n",
      "\u001b[0;31mTypeError\u001b[0m: split() missing 2 required positional arguments: 'temporal_features' and 'percent_train'"
     ]
    }
   ],
   "source": [
    "##### MAIN MODEL #####\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "window_size = 60\n",
    "step = 1\n",
    "predict_end_of_window = True\n",
    "\n",
    "batch_size = 4096 #8192\n",
    "num_epochs = 30\n",
    "verbose = 1\n",
    "loss = 'mean_squared_error'\n",
    "optimizer = 'adam'\n",
    "num_LSTM = 3\n",
    "num_units = [128, 256, 256]\n",
    "#num_LSTM = 2\n",
    "#num_units = [256, 256]\n",
    "dropout = 0.1\n",
    "\n",
    "path = 'cboe/parquet_preprocessed_BTCUSD_merged'\n",
    "day_start = 401\n",
    "day_end = None\n",
    "\n",
    "model_name = 'window-%s_step-%s_predEndWindow-%s_batch-%s_epochs-%s_loss-%s_opt-%s_numLSTMs-%s_numUnits-%s_dropout-%s_dayStart-%s_dayEnd-%s' % (window_size, step, str(predict_end_of_window), batch_size, num_epochs, \n",
    "                                                                                                                                                loss, optimizer, num_LSTM, num_units, dropout, str(day_start), str(day_end))\n",
    "print(model_name+'\\n')\n",
    "model_path = 'models/%s' % model_name\n",
    "\n",
    "# LOAD DATA\n",
    "df = load_data(path, day_start, day_end)\n",
    "\n",
    "# CREATE XY DATA\n",
    "X_train, X_dev, X_test, Y_train, Y_dev, Y_test = split(df)\n",
    "X_train, X_dev, X_test, Y_train, Y_dev, Y_test = create_all_XY(X_train, X_dev, X_test, Y_train, Y_dev, Y_test,\n",
    "                                                               window_size, step, predict_end_of_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE MODEL\n",
    "model = initialize_model(X_train, loss, optimizer, num_LSTM, num_units, dropout, predict_end_of_window=predict_end_of_window)\n",
    "\n",
    "# TRAIN MODEL\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs, \n",
    "                    validation_data=(X_dev, Y_dev), verbose=verbose, shuffle=False) \n",
    "\n",
    "# SAVE MODEL AND HISTORY\n",
    "save_model_history(model, history, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL AND HISTORY\n",
    "save_model_history(model, history, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE MODEL\n",
    "print('Evaluating...')\n",
    "evaluate = evaluate_model(model, history, X_train, X_dev, X_test, Y_train, Y_dev, Y_test, predict_end_of_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# VISUALIZE\n",
    "## Plot: \n",
    "# Historical price, color coded with train, dev, test\n",
    "# Historical percent change, color coded with train, dev, test\n",
    "# Train Loss, Dev Loss\n",
    "# Actual price vs predicted price (or percent change) for test set\n",
    "# Example features time series for one day (NOTE: in the preprocessing_final notebook)\n",
    "\n",
    "timestep_within_window = 0\n",
    "minute_start = 59\n",
    "minute_end = len(Y_test)\n",
    "\n",
    "print_save_events_props(Y_train.flatten(), Y_dev.flatten(), Y_test.flatten(), evaluate, model_name, model_path)\n",
    "#print_save_events_props(Y_train, Y_dev, Y_test, evaluate, model_name, model_path)\n",
    "plot_price(df, X_train, X_dev, field='current_price')\n",
    "plot_price(df, X_train, X_dev, field='percent_change')\n",
    "plot_train_dev_losses(history)\n",
    "plot_percent_change(evaluate['y_hat_test'], Y_test, timestep_within_window, minute_start, minute_end, predict_end_of_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window-60_step-1_predEndWindow-True_temporalFeat-True_batch-4096_epochs-30_loss-mean_squared_error_opt-adam_numLSTMs-3_numUnits-[128, 256, 256]_dropout-0.1_dayStart-401_dayEnd-None\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "split() missing 1 required positional argument: 'percent_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-cce54ce935d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# CREATE XY DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemporal_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m X_train, X_dev, X_test, Y_train, Y_dev, Y_test = create_all_XY(X_train, X_dev, X_test, Y_train, Y_dev, Y_test,\n\u001b[1;32m     34\u001b[0m                                                                window_size, step, predict_end_of_window)\n",
      "\u001b[0;31mTypeError\u001b[0m: split() missing 1 required positional argument: 'percent_train'"
     ]
    }
   ],
   "source": [
    "##### MAIN MODEL #####\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "window_size = 60\n",
    "step = 1\n",
    "predict_end_of_window = True\n",
    "temporal_features = True\n",
    "\n",
    "batch_size = 4096 #8192\n",
    "num_epochs = 30\n",
    "verbose = 1\n",
    "loss = 'mean_squared_error'\n",
    "optimizer = 'adam'\n",
    "num_LSTM = 3\n",
    "num_units = [128, 256, 256]\n",
    "#num_LSTM = 2\n",
    "#num_units = [256, 256]\n",
    "dropout = 0.1\n",
    "\n",
    "path = 'cboe/parquet_preprocessed_BTCUSD_merged'\n",
    "day_start = 401\n",
    "day_end = None\n",
    "\n",
    "model_name = 'window-%s_step-%s_predEndWindow-%s_temporalFeat-%s_batch-%s_epochs-%s_loss-%s_opt-%s_numLSTMs-%s_numUnits-%s_dropout-%s_dayStart-%s_dayEnd-%s' % (window_size, step, str(predict_end_of_window), str(temporal_features), batch_size, num_epochs, loss, optimizer, num_LSTM, num_units, dropout, str(day_start), str(day_end))\n",
    "print(model_name+'\\n')\n",
    "model_path = 'models/%s' % model_name\n",
    "\n",
    "# LOAD DATA\n",
    "df = load_data(path, day_start, day_end)\n",
    "\n",
    "# CREATE XY DATA\n",
    "X_train, X_dev, X_test, Y_train, Y_dev, Y_test = split(df, temporal_features)\n",
    "X_train, X_dev, X_test, Y_train, Y_dev, Y_test = create_all_XY(X_train, X_dev, X_test, Y_train, Y_dev, Y_test,\n",
    "                                                               window_size, step, predict_end_of_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE MODEL\n",
    "model = initialize_model(X_train, loss, optimizer, num_LSTM, num_units, dropout, predict_end_of_window=predict_end_of_window)\n",
    "\n",
    "# TRAIN MODEL\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs, \n",
    "                    validation_data=(X_dev, Y_dev), verbose=verbose, shuffle=False) \n",
    "\n",
    "# SAVE MODEL AND HISTORY\n",
    "save_model_history(model, history, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE MODEL\n",
    "print('Evaluating...')\n",
    "evaluate = evaluate_model(model, history, X_train, X_dev, X_test, Y_train, Y_dev, Y_test, predict_end_of_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# VISUALIZE\n",
    "## Plot: \n",
    "# Historical price, color coded with train, dev, test\n",
    "# Historical percent change, color coded with train, dev, test\n",
    "# Train Loss, Dev Loss\n",
    "# Actual price vs predicted price (or percent change) for test set\n",
    "# Example features time series for one day (NOTE: in the preprocessing_final notebook)\n",
    "\n",
    "timestep_within_window = 0\n",
    "minute_start = 59\n",
    "minute_end = len(Y_test)\n",
    "\n",
    "print_save_events_props(Y_train.flatten(), Y_dev.flatten(), Y_test.flatten(), evaluate, model_name, model_path)\n",
    "#print_save_events_props(Y_train, Y_dev, Y_test, evaluate, model_name, model_path)\n",
    "plot_price(df, X_train, X_dev, field='current_price')\n",
    "plot_price(df, X_train, X_dev, field='percent_change')\n",
    "plot_train_dev_losses(history)\n",
    "plot_percent_change(evaluate['y_hat_train'], Y_test, timestep_within_window, minute_start, minute_end, predict_end_of_window)\n",
    "plot_percent_change(evaluate['y_hat_dev'], Y_test, timestep_within_window, minute_start, minute_end, predict_end_of_window)\n",
    "plot_percent_change(evaluate['y_hat_test'], Y_test, timestep_within_window, minute_start, minute_end, predict_end_of_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window-60_step-1_predEndWindow-False_temporalFeat-True_batch-4096_epochs-30_loss-mean_squared_error_opt-adam_numLSTMs-3_numUnits-128-256-256_dropout-0.1_dayStart-401_dayEnd-None\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "split() missing 1 required positional argument: 'percent_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-e6f63b7530fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# CREATE XY DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemporal_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m X_train, X_dev, X_test, Y_train, Y_dev, Y_test = create_all_XY(X_train, X_dev, X_test, Y_train, Y_dev, Y_test,\n\u001b[1;32m     36\u001b[0m                                                                window_size, step, predict_end_of_window)\n",
      "\u001b[0;31mTypeError\u001b[0m: split() missing 1 required positional argument: 'percent_train'"
     ]
    }
   ],
   "source": [
    "##### MAIN MODEL #####\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "window_size = 60\n",
    "step = 1\n",
    "predict_end_of_window = False\n",
    "temporal_features = True\n",
    "\n",
    "batch_size = 4096 #8192\n",
    "num_epochs = 30\n",
    "verbose = 1\n",
    "loss = 'mean_squared_error'\n",
    "optimizer = 'adam'\n",
    "num_LSTM = 3\n",
    "num_units = [128, 256, 256]\n",
    "#num_LSTM = 2\n",
    "#num_units = [256, 256]\n",
    "dropout = 0.1\n",
    "\n",
    "path = 'cboe/parquet_preprocessed_BTCUSD_merged'\n",
    "day_start = 401\n",
    "day_end = None\n",
    "\n",
    "num_units_string = '-'.join([str(u) for u in num_units])\n",
    "\n",
    "model_name = 'window-%s_step-%s_predEndWindow-%s_temporalFeat-%s_batch-%s_epochs-%s_loss-%s_opt-%s_numLSTMs-%s_numUnits-%s_dropout-%s_dayStart-%s_dayEnd-%s' % (window_size, step, str(predict_end_of_window), str(temporal_features), batch_size, num_epochs, loss, optimizer, num_LSTM, num_units_string, dropout, str(day_start), str(day_end))\n",
    "print(model_name+'\\n')\n",
    "model_path = 'models/%s' % model_name  \n",
    "\n",
    "# LOAD DATA\n",
    "df = load_data(path, day_start, day_end)\n",
    "\n",
    "# CREATE XY DATA\n",
    "X_train, X_dev, X_test, Y_train, Y_dev, Y_test = split(df, temporal_features)\n",
    "X_train, X_dev, X_test, Y_train, Y_dev, Y_test = create_all_XY(X_train, X_dev, X_test, Y_train, Y_dev, Y_test,\n",
    "                                                               window_size, step, predict_end_of_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE MODEL\n",
    "model = initialize_model(X_train, loss, optimizer, num_LSTM, num_units, dropout, predict_end_of_window=predict_end_of_window)\n",
    "\n",
    "# TRAIN MODEL\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs, \n",
    "                    validation_data=(X_dev, Y_dev), verbose=verbose, shuffle=False) \n",
    "\n",
    "# SAVE MODEL AND HISTORY\n",
    "save_model_history(model, history, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE MODEL\n",
    "print('Evaluating...')\n",
    "evaluate = evaluate_model(model, history, X_train, X_dev, X_test, Y_train, Y_dev, Y_test, predict_end_of_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# VISUALIZE\n",
    "## Plot: \n",
    "# Historical price, color coded with train, dev, test\n",
    "# Historical percent change, color coded with train, dev, test\n",
    "# Train Loss, Dev Loss\n",
    "# Actual price vs predicted price (or percent change) for test set\n",
    "# Example features time series for one day (NOTE: in the preprocessing_final notebook)\n",
    "\n",
    "if predict_end_of_window:\n",
    "    timestep_within_window = 0\n",
    "else:\n",
    "    timestep_within_window = 29\n",
    "    \n",
    "minute_start = 0\n",
    "\n",
    "print_save_events_props(Y_train.flatten(), Y_dev.flatten(), Y_test.flatten(), evaluate, model_name, model_path)\n",
    "plot_price(df, X_train, X_dev, field='current_price')\n",
    "plot_price(df, X_train, X_dev, field='percent_change')\n",
    "plot_train_dev_losses(history)\n",
    "\n",
    "minute_end = len(Y_train)\n",
    "plot_percent_change(evaluate['y_hat_train'], Y_train, timestep_within_window, minute_start, minute_end, predict_end_of_window)\n",
    "\n",
    "minute_end = len(Y_dev)\n",
    "plot_percent_change(evaluate['y_hat_dev'], Y_dev, timestep_within_window, minute_start, minute_end, predict_end_of_window)\n",
    "\n",
    "minute_end = len(Y_test)\n",
    "plot_percent_change(evaluate['y_hat_test'], Y_test, timestep_within_window, minute_start, minute_end, predict_end_of_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window-60_step-1_predEndWindow-False_temporalFeat-False_batch-4096_epochs-30_loss-mean_squared_error_opt-adam_numLSTMs-3_numUnits-128-256-256_dropout-0.1_dayStart-785_dayEnd-None\n",
      "\n",
      "n_all:   116186\n",
      "n_train: 58093\n",
      "n_dev:   29046\n",
      "n_test:  29046\n",
      "(58093, 15)\n",
      "(29046, 15)\n",
      "(29047, 15)\n",
      "(58093, 1)\n",
      "(29046, 1)\n",
      "(29047, 1)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'create_all_XY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-028b320b04fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# CREATE XY DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemporal_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercent_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m X_train, X_dev, X_test, Y_train, Y_dev, Y_test = create_all_XY(X_train, X_dev, X_test, Y_train, Y_dev, Y_test,\n\u001b[0m\u001b[1;32m     37\u001b[0m                                                                window_size, step, predict_end_of_window)\n\u001b[1;32m     38\u001b[0m \u001b[0mplot_price\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'current_price'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_all_XY' is not defined"
     ]
    }
   ],
   "source": [
    "##### MAIN MODEL #####\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "window_size = 60\n",
    "step = 1\n",
    "predict_end_of_window = False\n",
    "temporal_features = False\n",
    "\n",
    "batch_size = 4096 #8192\n",
    "num_epochs = 30\n",
    "verbose = 1\n",
    "loss = 'mean_squared_error'\n",
    "optimizer = 'adam'\n",
    "num_LSTM = 3\n",
    "num_units = [128, 256, 256]\n",
    "#num_LSTM = 2\n",
    "#num_units = [256, 256]\n",
    "dropout = 0.1\n",
    "\n",
    "path = 'cboe/parquet_preprocessed_BTCUSD_merged'\n",
    "day_start = 785\n",
    "day_end = None\n",
    "percent_train = 0.5\n",
    "\n",
    "num_units_string = '-'.join([str(u) for u in num_units])\n",
    "\n",
    "model_name = 'window-%s_step-%s_predEndWindow-%s_temporalFeat-%s_batch-%s_epochs-%s_loss-%s_opt-%s_numLSTMs-%s_numUnits-%s_dropout-%s_dayStart-%s_dayEnd-%s' % (window_size, step, str(predict_end_of_window), str(temporal_features), batch_size, num_epochs, loss, optimizer, num_LSTM, num_units_string, dropout, str(day_start), str(day_end))\n",
    "print(model_name+'\\n')\n",
    "model_path = 'models/%s' % model_name  \n",
    "\n",
    "# LOAD DATA\n",
    "df = load_data(path, day_start, day_end)\n",
    "\n",
    "# CREATE XY DATA\n",
    "X_train, X_dev, X_test, Y_train, Y_dev, Y_test = split(df, temporal_features, percent_train)\n",
    "X_train, X_dev, X_test, Y_train, Y_dev, Y_test = create_all_XY(X_train, X_dev, X_test, Y_train, Y_dev, Y_test,\n",
    "                                                               window_size, step, predict_end_of_window)\n",
    "plot_price(df, X_train, X_dev, field='current_price')\n",
    "plot_price(df, X_train, X_dev, field='percent_change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE MODEL\n",
    "model = initialize_model(X_train, loss, optimizer, num_LSTM, num_units, dropout, predict_end_of_window=predict_end_of_window)\n",
    "\n",
    "# TRAIN MODEL\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs, \n",
    "                    validation_data=(X_dev, Y_dev), verbose=verbose, shuffle=False) \n",
    "\n",
    "# SAVE MODEL AND HISTORY\n",
    "save_model_history(model, history, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE MODEL\n",
    "model = initialize_model(X_train, loss, optimizer, num_LSTM, num_units, dropout, predict_end_of_window=predict_end_of_window)\n",
    "\n",
    "# TRAIN MODEL\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs, \n",
    "                    validation_data=(X_dev, Y_dev), callbacks=callbacks_list, verbose=verbose, shuffle=True) \n",
    "\n",
    "# SAVE MODEL AND HISTORY\n",
    "save_model_history(model, history, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE MODEL\n",
    "print('Evaluating...')\n",
    "evaluate = evaluate_model(model, history, X_train, X_dev, X_test, Y_train, Y_dev, Y_test, predict_end_of_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE\n",
    "## Plot: \n",
    "# Historical price, color coded with train, dev, test\n",
    "# Historical percent change, color coded with train, dev, test\n",
    "# Train Loss, Dev Loss\n",
    "# Actual price vs predicted price (or percent change) for test set\n",
    "# Example features time series for one day (NOTE: in the preprocessing_final notebook)\n",
    "\n",
    "if predict_end_of_window:\n",
    "    timestep_within_window = 0\n",
    "else:\n",
    "    timestep_within_window = 29\n",
    "    \n",
    "minute_start = 0\n",
    "\n",
    "print_save_events_props(Y_train.flatten(), Y_dev.flatten(), Y_test.flatten(), evaluate, model_name, model_path)\n",
    "plot_price(df, X_train, X_dev, field='current_price')\n",
    "plot_price(df, X_train, X_dev, field='percent_change')\n",
    "plot_train_dev_losses(history)\n",
    "\n",
    "minute_end = len(Y_train)\n",
    "plot_percent_change(evaluate['y_hat_train'], Y_train, timestep_within_window, minute_start, minute_end, predict_end_of_window)\n",
    "\n",
    "minute_end = len(Y_dev)\n",
    "plot_percent_change(evaluate['y_hat_dev'], Y_dev, timestep_within_window, minute_start, minute_end, predict_end_of_window)\n",
    "\n",
    "minute_end = len(Y_test)\n",
    "plot_percent_change(evaluate['y_hat_test'], Y_test, timestep_within_window, minute_start, minute_end, predict_end_of_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MAIN MODEL #####\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "window_size = 1440\n",
    "step = 1\n",
    "predict_end_of_window = False\n",
    "temporal_features = False\n",
    "\n",
    "batch_size = 128 #8192\n",
    "num_epochs = 15\n",
    "verbose = 1\n",
    "loss = 'mean_squared_error'\n",
    "optimizer = 'adam'\n",
    "num_LSTM = 3\n",
    "num_units = [256, 256, 256]\n",
    "#num_LSTM = 2\n",
    "#num_units = [256, 256]\n",
    "dropout = 0.1\n",
    "\n",
    "path = 'cboe/parquet_preprocessed_BTCUSD_merged'\n",
    "day_start = 785\n",
    "day_end = None\n",
    "percent_train = 0.6\n",
    "\n",
    "num_units_string = '-'.join([str(u) for u in num_units])\n",
    "\n",
    "model_name = 'window-%s_step-%s_predEndWindow-%s_temporalFeat-%s_batch-%s_epochs-%s_loss-%s_opt-%s_numLSTMs-%s_numUnits-%s_dropout-%s_dayStart-%s_dayEnd-%s' % (window_size, step, str(predict_end_of_window), str(temporal_features), batch_size, num_epochs, loss, optimizer, num_LSTM, num_units_string, dropout, str(day_start), str(day_end))\n",
    "print(model_name+'\\n')\n",
    "model_path = 'models/%s' % model_name  \n",
    "\n",
    "# LOAD DATA\n",
    "df = load_data(path, day_start, day_end)\n",
    "\n",
    "# CREATE XY DATA\n",
    "X_train, X_dev, X_test, Y_train, Y_dev, Y_test = split(df, temporal_features, percent_train)\n",
    "X_train, X_dev, X_test, Y_train, Y_dev, Y_test = create_all_XY(X_train, X_dev, X_test, Y_train, Y_dev, Y_test,\n",
    "                                                               window_size, step, predict_end_of_window)\n",
    "plot_price(df, X_train, X_dev, field='current_price')\n",
    "plot_price(df, X_train, X_dev, field='percent_change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE MODEL\n",
    "model = initialize_model(X_train, loss, optimizer, num_LSTM, num_units, dropout, predict_end_of_window=predict_end_of_window)\n",
    "\n",
    "# TRAIN MODEL\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs, \n",
    "                    validation_data=(X_dev, Y_dev), verbose=verbose, shuffle=False) \n",
    "\n",
    "# SAVE MODEL AND HISTORY\n",
    "save_model_history(model, history, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE MODEL\n",
    "print('Evaluating...')\n",
    "evaluate = evaluate_model(model, history, X_train, X_dev, X_test, Y_train, Y_dev, Y_test, predict_end_of_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE\n",
    "## Plot: \n",
    "# Historical price, color coded with train, dev, test\n",
    "# Historical percent change, color coded with train, dev, test\n",
    "# Train Loss, Dev Loss\n",
    "# Actual price vs predicted price (or percent change) for test set\n",
    "# Example features time series for one day (NOTE: in the preprocessing_final notebook)\n",
    "\n",
    "if predict_end_of_window:\n",
    "    timestep_within_window = 0\n",
    "else:\n",
    "    timestep_within_window = 29\n",
    "    \n",
    "minute_start = 0\n",
    "\n",
    "print_save_events_props(Y_train.flatten(), Y_dev.flatten(), Y_test.flatten(), evaluate, model_name, model_path)\n",
    "plot_price(df, X_train, X_dev, field='current_price')\n",
    "plot_price(df, X_train, X_dev, field='percent_change')\n",
    "plot_train_dev_losses(history)\n",
    "\n",
    "minute_end = len(Y_train)\n",
    "plot_percent_change(evaluate['y_hat_train'], Y_train, timestep_within_window, minute_start, minute_end, predict_end_of_window)\n",
    "\n",
    "minute_end = len(Y_dev)\n",
    "plot_percent_change(evaluate['y_hat_dev'], Y_dev, timestep_within_window, minute_start, minute_end, predict_end_of_window)\n",
    "\n",
    "minute_end = len(Y_test)\n",
    "plot_percent_change(evaluate['y_hat_test'], Y_test, timestep_within_window, minute_start, minute_end, predict_end_of_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = sorted(glob('models/*'))\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'models/window-60_step-1_predEndWindow-False_temporalFeat-True_batch-4096_epochs-30_loss-mean_squared_error_opt-adam_numLSTMs-3_numUnits-128-256-256_dropout-0.1_dayStart-401_dayEnd-None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_history = pickle.load( open( file+\"/trainHistoryDict\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.log(restored_history['loss'])\n",
    "new_df = pd.DataFrame(vals, columns=['loss'])\n",
    "title = 'Training Loss'\n",
    "new_df.plot(y = 'loss', figsize=(7,6), title=title, fontsize=14, legend=False, color='firebrick')\n",
    "plt.xlabel('Epoch', fontsize=18)\n",
    "plt.title(title, fontsize=15, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.log(restored_history['val_loss'])\n",
    "new_df = pd.DataFrame(vals, columns=['loss'])\n",
    "title = 'Dev Loss'\n",
    "new_df.plot(y = 'loss', figsize=(7,6), title=title, fontsize=14, legend=False, color='firebrick')\n",
    "plt.xlabel('Epoch', fontsize=18)\n",
    "plt.title(title, fontsize=15, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(restored_history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MAIN MODEL #####\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "window_size = 60\n",
    "step = 1\n",
    "predict_end_of_window = False\n",
    "temporal_features = True\n",
    "\n",
    "batch_size = 4096#2048 #8192\n",
    "num_epochs = 100\n",
    "verbose = 1\n",
    "loss = 'mean_squared_error'\n",
    "optimizer = 'adam'\n",
    "num_LSTM = 3\n",
    "num_units = [512, 256, 256]\n",
    "#num_LSTM = 2\n",
    "#num_units = [256, 256]\n",
    "dropout = 0.1\n",
    "\n",
    "path = 'cboe/parquet_preprocessed_BTCUSD_merged'\n",
    "day_start = 0\n",
    "day_end = None\n",
    "percent_train = 0.9\n",
    "\n",
    "num_units_string = '-'.join([str(u) for u in num_units])\n",
    "\n",
    "model_name = 'window-%s_step-%s_predEndWindow-%s_temporalFeat-%s_batch-%s_epochs-%s_loss-%s_opt-%s_numLSTMs-%s_numUnits-%s_dropout-%s_dayStart-%s_dayEnd-%s' % (window_size, step, str(predict_end_of_window), str(temporal_features), batch_size, num_epochs, loss, optimizer, num_LSTM, num_units_string, dropout, str(day_start), str(day_end))\n",
    "print(model_name+'\\n')\n",
    "model_path = 'models/%s' % model_name  \n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=15)\n",
    "callbacks_list = [early_stop]\n",
    "\n",
    "# LOAD DATA\n",
    "df = load_data(path, day_start, day_end)\n",
    "\n",
    "# CREATE XY DATA\n",
    "X_train, X_dev, X_test, Y_train, Y_dev, Y_test = split(df, temporal_features, percent_train)\n",
    "X_train, X_dev, X_test, Y_train, Y_dev, Y_test = create_all_XY(X_train, X_dev, X_test, Y_train, Y_dev, Y_test,\n",
    "                                                               window_size, step, predict_end_of_window)\n",
    "plot_price(df, X_train, X_dev, field='current_price')\n",
    "plot_price(df, X_train, X_dev, field='percent_change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# INITIALIZE MODEL\n",
    "model = initialize_model(X_train, loss, optimizer, num_LSTM, num_units, dropout, predict_end_of_window=predict_end_of_window)\n",
    "\n",
    "# TRAIN MODEL\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs, \n",
    "                    validation_data=(X_dev, Y_dev), callbacks=callbacks_list, verbose=verbose, shuffle=True) \n",
    "\n",
    "# SAVE MODEL AND HISTORY\n",
    "save_model_history(model, history, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE MODEL\n",
    "print('Evaluating...')\n",
    "evaluate = evaluate_model(model, history, X_train, X_dev, X_test, Y_train, Y_dev, Y_test, predict_end_of_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE\n",
    "## Plot: \n",
    "# Historical price, color coded with train, dev, test\n",
    "# Historical percent change, color coded with train, dev, test\n",
    "# Train Loss, Dev Loss\n",
    "# Actual price vs predicted price (or percent change) for test set\n",
    "# Example features time series for one day (NOTE: in the preprocessing_final notebook)\n",
    "\n",
    "if predict_end_of_window:\n",
    "    timestep_within_window = 0\n",
    "else:\n",
    "    timestep_within_window = window-1\n",
    "    \n",
    "minute_start = 0\n",
    "\n",
    "print_save_events_props(Y_train.flatten(), Y_dev.flatten(), Y_test.flatten(), evaluate, model_name, model_path)\n",
    "plot_price(df, X_train, X_dev, field='current_price')\n",
    "plot_price(df, X_train, X_dev, field='percent_change')\n",
    "plot_train_dev_losses(history)\n",
    "\n",
    "minute_end = len(Y_train)\n",
    "plot_percent_change(evaluate['y_hat_train'], Y_train, timestep_within_window, minute_start, minute_end, predict_end_of_window)\n",
    "\n",
    "minute_end = len(Y_dev)\n",
    "plot_percent_change(evaluate['y_hat_dev'], Y_dev, timestep_within_window, minute_start, minute_end, predict_end_of_window)\n",
    "\n",
    "minute_end = len(Y_test)\n",
    "plot_percent_change(evaluate['y_hat_test'], Y_test, timestep_within_window, minute_start, minute_end, predict_end_of_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(time_series):\n",
    "    total_squared_error = 0\n",
    "    total_absolute_error = 0\n",
    "    prev_obv = time_series[0]\n",
    "    for obv in time_series[1:]:\n",
    "        total_squared_error += (obv - prev_obv)**2\n",
    "        total_absolute_error += abs(obv - prev_obv)\n",
    "        prev_obv = obv\n",
    "    num_predictions = len(time_series) - 1\n",
    "    mean_squared_error = total_squared_error / num_predictions\n",
    "    mean_absolute_error = total_absolute_error / num_predictions\n",
    "    root_mean_squared_error = np.sqrt(mean_squared_error)\n",
    "    print('=== baseline ===')\n",
    "    print('total squared error')\n",
    "    print(total_squared_error)\n",
    "    print('total absolute error')\n",
    "    print(total_absolute_error)\n",
    "    print('mean squared error')\n",
    "    print(mean_squared_error)\n",
    "    print('mean absolute error')\n",
    "    print(mean_absolute_error) \n",
    "    print('root mean squared error')\n",
    "    print(root_mean_squared_error) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "df = load_data(path, day_start, day_end)\n",
    "all_Y = df['current_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-40c6d7958b9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-08b1a91b744f>\u001b[0m in \u001b[0;36mmse\u001b[0;34m(time_series)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprev_obv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_series\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mobv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtime_series\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtotal_squared_error\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobv\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprev_obv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtotal_absolute_error\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobv\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprev_obv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprev_obv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;31m# this makes sure that we are aligned like the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(left, right, name, na_op)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_align_method_SERIES\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m         \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mget_op\u001b[0;34m(cls, left, right, name, na_op)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \"\"\"\n\u001b[1;32m    342\u001b[0m         \u001b[0mis_timedelta_lhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_timedelta64_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         is_datetime_lhs = (is_datetime64_dtype(left) or\n\u001b[0m\u001b[1;32m    344\u001b[0m                            is_datetime64tz_dtype(left))\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_datetime64_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0mtipo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_dtype_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36m_get_dtype_type\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1789\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1790\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr_or_dtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1791\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1792\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1793\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mse(all_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 185.13336327386506\n",
      "RMSE: 13.6063721569662\n"
     ]
    }
   ],
   "source": [
    "MSE = np.sum((np.array(all_Y[:-1])-np.array(all_Y[1:]))**2) / len(all_Y[:-1])\n",
    "RMSE = np.sqrt(MSE)\n",
    "print('MSE:', MSE)\n",
    "print('RMSE:', RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29047, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_all:   116186\n",
      "n_train: 58093\n",
      "n_dev:   29046\n",
      "n_test:  29046\n",
      "(58093, 15)\n",
      "(29046, 15)\n",
      "(29047, 15)\n",
      "(58093, 1)\n",
      "(29046, 1)\n",
      "(29047, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_dev, X_test, Y_train, Y_dev, Y_test = split(df, temporal_features, percent_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.1971543448068038e-05\n",
      "RMSE: 0.0034599918277458458\n"
     ]
    }
   ],
   "source": [
    "Y = Y_train\n",
    "MSE = np.sum((np.array(Y[:-1])-np.array(Y[1:]))**2) / len(Y[:-1])\n",
    "RMSE = np.sqrt(MSE)\n",
    "print('MSE:', MSE)\n",
    "print('RMSE:', RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.246933067782772e-05\n",
      "RMSE: 0.003531193945088222\n"
     ]
    }
   ],
   "source": [
    "Y = Y_dev\n",
    "MSE = np.sum((np.array(Y[:-1])-np.array(Y[1:]))**2) / len(Y[:-1])\n",
    "RMSE = np.sqrt(MSE)\n",
    "print('MSE:', MSE)\n",
    "print('RMSE:', RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.3101213170273331e-05\n",
      "RMSE: 0.003619559803384015\n"
     ]
    }
   ],
   "source": [
    "Y = Y_test\n",
    "MSE = np.sum((np.array(Y[:-1])-np.array(Y[1:]))**2) / len(Y[:-1])\n",
    "RMSE = np.sqrt(MSE)\n",
    "print('MSE:', MSE)\n",
    "print('RMSE:', RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "losss = 1.799e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001341268056728408"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(losss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#per row: train, dev, test loss\n",
    "losss = np.array([[1.690e-6, 5.907e-6, 5.893e-6],\n",
    "[1.673e-6, 5.79e-6, 5.865e-6],\n",
    "[1.925e-6, 1.043e-5, 1.18e-6],\n",
    "[1.959e-6, 5.909e-6, 5.97e-6],\n",
    "[1.694e-6, 6.255e-6, 6.31e-6],\n",
    "[1.799e-6, 5.791e-6, 5.94e-6],\n",
    "[1.822e-6, 1.00e-5, 1.09e-5],\n",
    "[1.718e-5, 6.934e-5, 6.988e-5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00192815, 0.00386364, 0.00374062]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(losss.mean(axis=0, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-1f8a688cae5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x7f2ad3ba7dd8>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58093, 1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MAIN MODEL #####\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "window_size = 60\n",
    "step = 1\n",
    "predict_end_of_window = False\n",
    "temporal_features = True\n",
    "\n",
    "batch_size = 256#2048 #8192\n",
    "num_epochs = 2\n",
    "verbose = 1\n",
    "loss = 'mean_squared_error'\n",
    "optimizer = 'adam'\n",
    "#num_LSTM = 3\n",
    "#num_units = [512, 256, 256]\n",
    "num_LSTM = 2\n",
    "num_units = [256, 256]\n",
    "dropout = 0.1\n",
    "\n",
    "path = 'cboe/parquet_preprocessed_BTCUSD_merged'\n",
    "day_start = 800\n",
    "day_end = None\n",
    "percent_train = 0.9\n",
    "\n",
    "num_units_string = '-'.join([str(u) for u in num_units])\n",
    "\n",
    "model_name = 'window-%s_step-%s_predEndWindow-%s_temporalFeat-%s_batch-%s_epochs-%s_loss-%s_opt-%s_numLSTMs-%s_numUnits-%s_dropout-%s_dayStart-%s_dayEnd-%s' % (window_size, step, str(predict_end_of_window), str(temporal_features), batch_size, num_epochs, loss, optimizer, num_LSTM, num_units_string, dropout, str(day_start), str(day_end))\n",
    "print(model_name+'\\n')\n",
    "model_path = 'models/%s' % model_name  \n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=15)\n",
    "callbacks_list = [early_stop]\n",
    "\n",
    "# LOAD DATA\n",
    "df = load_data(path, day_start, day_end)\n",
    "\n",
    "# CREATE XY DATA\n",
    "X_train, X_dev, X_test, Y_train, Y_dev, Y_test = split(df, temporal_features, percent_train)\n",
    "X_train, X_dev, X_test, Y_train, Y_dev, Y_test = create_all_XY(X_train, X_dev, X_test, Y_train, Y_dev, Y_test,\n",
    "                                                               window_size, step, predict_end_of_window)\n",
    "plot_price(df, X_train, X_dev, field='current_price')\n",
    "plot_price(df, X_train, X_dev, field='percent_change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE MODEL\n",
    "model = initialize_model(X_train, loss, optimizer, num_LSTM, num_units, dropout, predict_end_of_window=predict_end_of_window)\n",
    "\n",
    "# TRAIN MODEL\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs, \n",
    "                    validation_data=(X_dev, Y_dev), callbacks=callbacks_list, verbose=verbose, shuffle=True) \n",
    "\n",
    "# SAVE MODEL AND HISTORY\n",
    "save_model_history(model, history, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
