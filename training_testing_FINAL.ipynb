{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import pandas as pd\n",
    "import lz4.frame\n",
    "import gzip\n",
    "import io\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from plumbum.cmd import rm\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import pickle\n",
    "import datetime\n",
    "import re\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotline(data):\n",
    "    plt.figure()\n",
    "    plt.plot(data)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def event_count(time_series, data_name):\n",
    "    time_series = time_series[['Fill Price (USD)']].values\n",
    "    upevents = 0\n",
    "    downevents = 0\n",
    "    sameprice = 0\n",
    "    prev_obv = time_series[0]\n",
    "    for obv in time_series[1:]:\n",
    "        if obv > prev_obv:\n",
    "            upevents += 1\n",
    "        elif obv < prev_obv:\n",
    "            downevents += 1\n",
    "        elif obv == prev_obv:\n",
    "            sameprice += 1\n",
    "        prev_obv = obv\n",
    "    print('=== Event counts on %s ===' % data_name)\n",
    "    print('upevents')\n",
    "    print(upevents)\n",
    "    print('downevents')\n",
    "    print(downevents)\n",
    "    print('sameprice')\n",
    "    print(sameprice)\n",
    "    print()\n",
    "\n",
    "def mse(time_series, data_name):\n",
    "    time_series = time_series[['Fill Price (USD)']].values\n",
    "    total_squared_error = 0\n",
    "    total_absolute_error = 0\n",
    "    prev_obv = time_series[0]\n",
    "    for obv in time_series[1:]:\n",
    "        total_squared_error += (obv - prev_obv)**2\n",
    "        total_absolute_error += abs(obv - prev_obv)\n",
    "        prev_obv = obv\n",
    "    num_predictions = len(time_series) - 1\n",
    "    mean_squared_error = total_squared_error / num_predictions\n",
    "    mean_absolute_error = total_absolute_error / num_predictions\n",
    "    root_mean_squared_error = np.sqrt(mean_squared_error)\n",
    "    print('=== baseline on %s ===' % data_name)\n",
    "    print('total squared error')\n",
    "    print(total_squared_error)\n",
    "    print('total absolute error')\n",
    "    print(total_absolute_error)\n",
    "    print('mean squared error')\n",
    "    print(mean_squared_error)\n",
    "    print('mean absolute error')\n",
    "    print(mean_absolute_error) \n",
    "    print('root mean squared error')\n",
    "    print(root_mean_squared_error) \n",
    "    print()\n",
    "    \n",
    "def show_summary_statistics():\n",
    "    #event_count(small_set, 'small')\n",
    "    train_set = df.iloc[0:num_samples_training]\n",
    "    dev_set = df.iloc[num_samples_training:num_samples_training+num_samples_dev]\n",
    "    test_set = df.iloc[num_samples_training+num_samples_dev:]\n",
    "    event_count(train_set, 'train')\n",
    "    event_count(dev_set, 'dev')\n",
    "    event_count(test_set, 'test')\n",
    "    mse(train_set, 'train')\n",
    "    mse(dev_set, 'dev')\n",
    "    mse(test_set, 'test')\n",
    "#show_summary_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, day_start=None, day_end=None):\n",
    "\n",
    "    # Concatenate dataframes\n",
    "    files = sorted(glob('%s/*.parquet' % path))\n",
    "    \n",
    "    if day_start is not None:\n",
    "        start = day_start\n",
    "    else:\n",
    "        start = 0\n",
    "    if day_end is not None:\n",
    "        end = day_start\n",
    "    else:\n",
    "        end = len(files)\n",
    "    files = files[start:end]\n",
    "    \n",
    "    all_dataframes = []\n",
    "    for file in files:\n",
    "        df = pq.read_table(file).to_pandas()\n",
    "        all_dataframes.append(df)\n",
    "    df = pd.concat(all_dataframes)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df, temporal_features, percent_train):\n",
    "    X_train, X_dev, X_test, train_prices, dev_prices, test_prices = split_X(df, temporal_features, percent_train)\n",
    "    Y_train, Y_dev, Y_test = split_Y(df, percent_train)\n",
    "    return X_train, X_dev, X_test, Y_train, Y_dev, Y_test, train_prices, dev_prices, test_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_X(df, temporal_features, percent_train):\n",
    "    n_all = df.shape[0]\n",
    "    n_train = round(n_all * percent_train)\n",
    "    n_dev   = round(n_all * ((1 - percent_train)/2))\n",
    "    n_test  = round(n_all * ((1 - percent_train)/2))\n",
    "    print('n_all:  ', n_all)\n",
    "    print('n_train:', n_train)\n",
    "    print('n_dev:  ', n_dev)\n",
    "    print('n_test: ', n_test)\n",
    "    \n",
    "    if temporal_features:\n",
    "        end = 59\n",
    "    else:\n",
    "        end = 16\n",
    "\n",
    "    X_train = df.iloc[:n_train, 1:end].values.astype('float32')\n",
    "    X_dev   = df.iloc[n_train:n_train+n_dev, 1:end].values.astype('float32')\n",
    "    X_test  = df.iloc[n_train+n_dev:, 1:end].values.astype('float32')\n",
    "    print(X_train.shape)\n",
    "    print(X_dev.shape)\n",
    "    print(X_test.shape)\n",
    "    \n",
    "    train_prices = df.iloc[:n_train, 0].values.astype('float32')\n",
    "    dev_prices   = df.iloc[n_train:n_train+n_dev, 0].values.astype('float32')\n",
    "    test_prices  = df.iloc[n_train+n_dev:, 0].values.astype('float32')\n",
    "\n",
    "    return X_train, X_dev, X_test, train_prices, dev_prices, test_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_Y(df, percent_train):\n",
    "    n_all = df.shape[0]\n",
    "    n_train = round(n_all * percent_train)\n",
    "    n_dev   = round(n_all * ((1 - percent_train)/2))\n",
    "    n_test  = round(n_all * ((1 - percent_train)/2))\n",
    "    \n",
    "    Y_train = df.iloc[:n_train, -1:].values.astype('float32')\n",
    "    Y_dev   = df.iloc[n_train:n_train+n_dev, -1:].values.astype('float32')\n",
    "    Y_test  = df.iloc[n_train+n_dev:, -1:].values.astype('float32')\n",
    "    print(Y_train.shape)\n",
    "    print(Y_dev.shape)\n",
    "    print(Y_test.shape)\n",
    "    \n",
    "    return Y_train, Y_dev, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_end_of_window_Y(Y, window, step):\n",
    "    # Returns label at the end of a given window\n",
    "    return np.array([Y[i] for i in range(window-1, len(Y), step)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_XY(X_train, X_dev, X_test, Y_train, Y_dev, Y_test, window_size, step, predict_end_of_window):\n",
    "    X_train = create_sequenced_data(X_train, window=window_size, step=step)\n",
    "    X_dev   = create_sequenced_data(X_dev,   window=window_size, step=step)\n",
    "    X_test  = create_sequenced_data(X_test,  window=window_size, step=step)\n",
    "\n",
    "    if predict_end_of_window:\n",
    "        Y_train = create_end_of_window_Y(Y_train, window=window_size, step=step)\n",
    "        Y_dev   = create_end_of_window_Y(Y_dev,   window=window_size, step=step)\n",
    "        Y_test  = create_end_of_window_Y(Y_test,  window=window_size, step=step)\n",
    "    else:\n",
    "        Y_train = create_sequenced_data(Y_train, window=window_size, step=step)\n",
    "        Y_dev   = create_sequenced_data(Y_dev,   window=window_size, step=step)\n",
    "        Y_test  = create_sequenced_data(Y_test,  window=window_size, step=step)\n",
    "    \n",
    "    print('Train, dev, test shapes:')\n",
    "    print(X_train.shape)\n",
    "    print(X_dev.shape)\n",
    "    print(X_test.shape)\n",
    "    print(Y_train.shape)\n",
    "    print(Y_dev.shape)\n",
    "    print(Y_test.shape)\n",
    "    \n",
    "    return X_train, X_dev, X_test, Y_train, Y_dev, Y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequenced_data(data, window, step):\n",
    "    sequenced = []\n",
    "    for minute in range(0, len(data) - window + 1, step):\n",
    "        chunk = data[minute:minute+window]\n",
    "        sequenced.append(chunk)\n",
    "    sequenced = np.array(sequenced)\n",
    "    return sequenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_parquet(df, outfile):\n",
    "    pq.write_table(pa.Table.from_pandas(df), outfile, compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direction_prediction(y_true, y_pred, predict_end_of_window):\n",
    "    if predict_end_of_window:\n",
    "        prop_correct = np.sum(np.sign(y_pred) == np.sign(y_true)) / y_true.shape[0]\n",
    "    else:\n",
    "        prop_correct = np.sum(np.sign(y_pred) == np.sign(y_true)) / (y_true.shape[0] * y_true.shape[1])\n",
    "    return prop_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_diffs(prices, y_hats):\n",
    "  # Predict within $x of actual price on average\n",
    "  price_diffs = []\n",
    "  for i in range(len(prices) - 1):\n",
    "    current_price       = prices[i]\n",
    "    percent_change_pred = y_hats[i]\n",
    "    next_pred_price     = current_price + (current_price * percent_change_pred)\n",
    "    next_true_price     = prices[i + 1]\n",
    "    price_diffs.append(np.abs(next_pred_price - next_true_price))\n",
    "  return price_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(X_train, loss, optimizer, num_LSTMs, num_units, dropout, predict_end_of_window):\n",
    "    \n",
    "    LSTM_input_shape = [X_train.shape[1], X_train.shape[2]]\n",
    "\n",
    "    # DEFINE MODEL\n",
    "    model = Sequential()\n",
    "\n",
    "    if num_LSTMs == 2:\n",
    "            model.add(LSTM(num_units[0], input_shape=LSTM_input_shape, return_sequences=True))\n",
    "            model.add(Dropout(dropout))\n",
    "            \n",
    "            if predict_end_of_window:\n",
    "                model.add(LSTM(num_units[1], return_sequences=False))\n",
    "            else:\n",
    "                model.add(LSTM(num_units[1], return_sequences=True))\n",
    "        \n",
    "    if num_LSTMs == 3:\n",
    "            model.add(LSTM(num_units[0], input_shape=LSTM_input_shape, return_sequences=True))\n",
    "            model.add(Dropout(dropout))\n",
    "\n",
    "            model.add(LSTM(num_units[1], return_sequences=True))\n",
    "            model.add(Dropout(dropout))\n",
    "            \n",
    "            if predict_end_of_window:\n",
    "                model.add(LSTM(num_units[2], return_sequences=False))\n",
    "            else:\n",
    "                model.add(LSTM(num_units[2], return_sequences=True))\n",
    "\n",
    "    if predict_end_of_window:\n",
    "        model.add(Dense(1))\n",
    "    else:\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "      \n",
    "    model.add(Activation('linear'))\n",
    "    \n",
    "    \n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, history, prices, X_train, X_dev, X_test, Y_train, Y_dev, Y_test, \n",
    "                   train_prices, dev_prices, test_prices, predict_end_of_window):\n",
    "  \n",
    "    train_loss = history.history['loss'][-1]\n",
    "    dev_loss   = history.history['val_loss'][-1]\n",
    "    \n",
    "    print('Evaluating test loss...')\n",
    "    test_loss  = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    \n",
    "    print('Predicting y_hat_train...')\n",
    "    y_hat_train = model.predict(X_train)\n",
    "    print('Predicting y_hat_dev...')\n",
    "    y_hat_dev   = model.predict(X_dev)\n",
    "    print('Predicting y_hat_test...')\n",
    "    y_hat_test  = model.predict(X_test)\n",
    "    \n",
    "    train_mse, train_rmse = mse_rmse(Y_train, y_hat_train)\n",
    "    dev_mse,   dev_rmse   = mse_rmse(Y_dev,   y_hat_dev)\n",
    "    test_mse,  test_rmse  = mse_rmse(Y_test,  y_hat_test)\n",
    "    \n",
    "    train_prop_correct = direction_prediction(Y_train, y_hat_train, predict_end_of_window)\n",
    "    dev_prop_correct   = direction_prediction(Y_dev,   y_hat_dev,   predict_end_of_window)\n",
    "    test_prop_correct  = direction_prediction(Y_test,  y_hat_test,  predict_end_of_window)\n",
    "    \n",
    "    price_diffs_train = price_diffs(train_prices, y_hat_train)\n",
    "    price_diffs_dev   = price_diffs(dev_prices,   y_hat_dev)\n",
    "    price_diffs_test  = price_diffs(test_prices,  y_hat_test)\n",
    "    \n",
    "    evaluation = {'train_loss': train_loss,\n",
    "                  'dev_loss':   dev_loss,\n",
    "                  'test_loss':  test_loss,\n",
    "                  \n",
    "                  'train_mse':  train_mse,\n",
    "                  'train_rmse': train_rmse,\n",
    "                  'dev_mse':    dev_mse,\n",
    "                  'dev_rmse':   dev_rmse,\n",
    "                  'test_mse':   test_mse,\n",
    "                  'test_rmse':  test_rmse,\n",
    "                  \n",
    "                  'train_prop_correct': train_prop_correct,\n",
    "                  'dev_prop_correct':   dev_prop_correct,\n",
    "                  'test_prop_correct':  test_prop_correct,\n",
    "                  \n",
    "                  'y_hat_train': y_hat_train,\n",
    "                  'y_hat_dev':   y_hat_dev,\n",
    "                  'y_hat_test':  y_hat_test}\n",
    "\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_history(model, history, model_path):\n",
    "  \n",
    "    # serialize model to JSON\n",
    "    if os.path.exists(model_path):\n",
    "        suffix = ''.join(re.findall(r'\\d+', str(datetime.datetime.now())))\n",
    "        model_path = model_path + '_' + suffix\n",
    "\n",
    "    os.makedirs(model_path)\n",
    "    \n",
    "    model_json = model.to_json()   \n",
    "    with open(model_path + '/model.json', 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    \n",
    "    try:\n",
    "      model.save_weights(model_path + '/model.h5')\n",
    "    except:\n",
    "      print('WARNING: Could not save weights...')\n",
    "    \n",
    "    with open(model_path + '/trainHistoryDict', 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "        \n",
    "    print(\"Saved model and history to:\\n%s\" % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_price(df, X_train, X_dev, field):\n",
    "    X_train_stop = len(X_train)\n",
    "    X_dev_stop = X_train_stop + len(X_dev)\n",
    "\n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.plot(np.arange(0, X_train_stop), df.iloc[0:X_train_stop][field], 'k')\n",
    "    plt.plot(np.arange(X_train_stop, X_dev_stop), df.iloc[X_train_stop:X_dev_stop][field], 'r')\n",
    "    plt.plot(np.arange(X_dev_stop, len(df)), df.iloc[X_dev_stop:len(df)][field], 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_dev_losses(history):\n",
    "    train_loss = history.history['loss']\n",
    "    dev_loss   = history.history['val_loss']\n",
    "    \n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.plot(np.log(train_loss), 'k')\n",
    "    \n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.plot(np.log(dev_loss), 'b')\n",
    "    \n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.plot(train_loss, 'k')\n",
    "    \n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.plot(dev_loss, 'b')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_percent_change(y_pred, y_true, timestep_within_window, minute_start, minute_end, predict_end_of_window):\n",
    "    \n",
    "    ys=[]\n",
    "    for i in range(len(y_pred)):\n",
    "        ys.append(y_pred[i][timestep_within_window])\n",
    "\n",
    "    original_ys=[]\n",
    "    for i in range(len(y_true)):\n",
    "        original_ys.append(y_true[i][timestep_within_window])\n",
    " \n",
    "    ys_orig = np.array(original_ys)\n",
    "    ys_pred = np.array(ys)\n",
    "    \n",
    "    OldRange = (ys_pred.max() - ys_pred.min())  \n",
    "    NewRange = (ys_orig.max() - ys_orig.min())   \n",
    "    new_ys_pred = (((ys - ys_pred.min()) * NewRange) / OldRange) + ys_orig.min()\n",
    "    \n",
    "    norm1 = ys_orig / np.linalg.norm(ys_orig)\n",
    "    norm2 = ys_pred / np.linalg.norm(ys_pred)\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(norm1[minute_start:minute_end], 'k', alpha=0.9)\n",
    "    plt.plot(norm2[minute_start:minute_end], 'r', alpha=0.9)\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(original_ys[minute_start:minute_end], 'k', alpha=0.9)\n",
    "    plt.plot(ys[minute_start:minute_end], 'r', alpha=0.9)\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(original_ys[minute_start:minute_end], 'k', alpha=0.9)\n",
    "    plt.plot(new_ys_pred[minute_start:minute_end], 'r', alpha=0.9)\n",
    "    \n",
    "    try:\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.plot(y_true[minute_start:minute_end], 'k', alpha=0.9)\n",
    "        plt.plot(y_pred[minute_start:minute_end], 'r', alpha=0.9)\n",
    "    except:\n",
    "        a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_save_events_props(train, dev, test, evaluate, model_name, model_path):\n",
    "    \n",
    "    train_event_counts = np.unique(np.sign(train), return_counts=True)\n",
    "    train_event_prop   = train_event_counts[1] / len(train)\n",
    "    \n",
    "    dev_event_counts = np.unique(np.sign(dev), return_counts=True)\n",
    "    dev_event_prop   = dev_event_counts[1] / len(dev)\n",
    "    \n",
    "    test_event_counts = np.unique(np.sign(test), return_counts=True)\n",
    "    test_event_prop   = test_event_counts[1] / len(test)\n",
    "    \n",
    "    print(model_name)\n",
    "    print('\\n========== EVENT COUNTS AND PROPORTIONS ==========')\n",
    "    print('=== TRAIN ===')\n",
    "    print('Down, Same, Up:', train_event_counts[1])\n",
    "    print('Down, Same, Up:', train_event_prop)\n",
    "    \n",
    "    print('\\n=== DEV ===')\n",
    "    print('Down, Same, Up:', dev_event_counts[1])\n",
    "    print('Down, Same, Up:', dev_event_prop)\n",
    "    \n",
    "    print('\\n=== TEST ===')\n",
    "    print('Down, Same, Up:', test_event_counts[1])\n",
    "    print('Down, Same, Up:', test_event_prop)\n",
    "    \n",
    "    \n",
    "    print('\\n========== CORRECTION DIRECTION PREDICTIONS ==========')\n",
    "    print(\"TRAIN: %f\\nDEV:   %f\\nTEST:  %f\" % (evaluate['train_prop_correct'], \n",
    "                                               evaluate['dev_prop_correct'], \n",
    "                                               evaluate['test_prop_correct']))\n",
    "    \n",
    "    print('\\n========== FINAL LOSS ==========')\n",
    "    print(\"TRAIN: %s\\nDEV:   %s\\nTEST:  %s\\n\" % (evaluate['train_loss'], evaluate['dev_loss'], evaluate['test_loss']))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history, field, title):\n",
    "  vals = np.log(restored_history[field])\n",
    "  #vals = restored_history[field]\n",
    "  new_df = pd.DataFrame(vals, columns=[field])\n",
    "  new_df.plot(y = field, figsize=(7,6), title=title, fontsize=14, legend=False, color='firebrick')\n",
    "  plt.xlabel('Epoch', fontsize=18)\n",
    "  plt.title(title, fontsize=15, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(time_series):\n",
    "    total_squared_error = 0\n",
    "    total_absolute_error = 0\n",
    "    prev_obv = time_series[0]\n",
    "    for obv in time_series[1:]:\n",
    "        total_squared_error += (obv - prev_obv)**2\n",
    "        total_absolute_error += abs(obv - prev_obv)\n",
    "        prev_obv = obv\n",
    "    num_predictions = len(time_series) - 1\n",
    "    mean_squared_error = total_squared_error / num_predictions\n",
    "    mean_absolute_error = total_absolute_error / num_predictions\n",
    "    root_mean_squared_error = np.sqrt(mean_squared_error)\n",
    "    print('=== baseline ===')\n",
    "    print('total squared error')\n",
    "    print(total_squared_error)\n",
    "    print('total absolute error')\n",
    "    print(total_absolute_error)\n",
    "    print('mean squared error')\n",
    "    print(mean_squared_error)\n",
    "    print('mean absolute error')\n",
    "    print(mean_absolute_error) \n",
    "    print('root mean squared error')\n",
    "    print(root_mean_squared_error) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_rmse(Y_true, Y_pred, verbose=False, baseline_desc=None):\n",
    "  MSE = np.sum((Y_true - Y_pred) ** 2) / len(Y_true)\n",
    "  RMSE = np.sqrt(MSE)\n",
    "  \n",
    "  if verbose:\n",
    "    print('\\n%s' % baseline_desc)\n",
    "    print('MSE:  %06f' % MSE)\n",
    "    print('RMSE: %06f' % RMSE)\n",
    "  \n",
    "  return MSE, RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baselines(Y, data_name):\n",
    "  print('\\n========= %s =========' % data_name)\n",
    "  Y_true = np.array(Y[1:])\n",
    "  \n",
    "  # Predict no percent change\n",
    "  Y_pred = np.zeros((Y_true.shape))\n",
    "  mse_rmse(Y_true, Y_pred, verbose=True, baseline_desc='Predict NO Percent Change')\n",
    "  \n",
    "  # Predict same percent change\n",
    "  Y_pred = np.array(Y[:-1])\n",
    "  mse_rmse(Y_true, Y_pred, verbose=True, baseline_desc='Predict SAME Percent Change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### MAIN MODEL #####\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "window_size = 60\n",
    "step = 1\n",
    "predict_end_of_window = False\n",
    "temporal_features = False\n",
    "\n",
    "batch_size = 2048 #8192\n",
    "num_epochs = 15\n",
    "#num_epochs = 30\n",
    "verbose = 1\n",
    "loss = 'mean_squared_error'\n",
    "optimizer = 'adam'\n",
    "num_LSTM = 3\n",
    "num_units = [128, 256, 256]\n",
    "#num_LSTM = 2\n",
    "#num_units = [256, 256]\n",
    "dropout = 0.1\n",
    "\n",
    "path = 'cboe/parquet_preprocessed_BTCUSD_merged'\n",
    "day_start = 401\n",
    "day_end = None\n",
    "percent_train = 0.9\n",
    "\n",
    "num_units_string = '-'.join([str(u) for u in num_units])\n",
    "\n",
    "model_name = 'window-%s_step-%s_predEndWindow-%s_temporalFeat-%s_batch-%s_epochs-%s_loss-%s_opt-%s_numLSTMs-%s_numUnits-%s_dropout-%s_dayStart-%s_dayEnd-%s' % (window_size, step, str(predict_end_of_window), str(temporal_features), batch_size, num_epochs, loss, optimizer, num_LSTM, num_units_string, dropout, str(day_start), str(day_end))\n",
    "print(model_name+'\\n')\n",
    "model_path = 'models/%s' % model_name  \n",
    "\n",
    "min_delta = 0.1\n",
    "patience = 15\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=min_delta, patience=patience)\n",
    "callbacks_list = [early_stop]\n",
    "\n",
    "# LOAD DATA\n",
    "df = load_data(path, day_start, day_end)\n",
    "\n",
    "# CREATE XY DATA\n",
    "X_train, X_dev, X_test, Y_train, Y_dev, Y_test, train_prices, dev_prices, test_prices = split(df, temporal_features, percent_train)\n",
    "X_train, X_dev, X_test, Y_train, Y_dev, Y_test = create_all_XY(X_train, X_dev, X_test, Y_train, Y_dev, Y_test,\n",
    "                                                               window_size, step, predict_end_of_window)\n",
    "plot_price(df, X_train, X_dev, field='current_price')\n",
    "plot_price(df, X_train, X_dev, field='percent_change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE MODEL\n",
    "print('Initializing model...')\n",
    "model = initialize_model(X_train, loss, optimizer, num_LSTM, num_units, dropout, \n",
    "                         predict_end_of_window=predict_end_of_window)\n",
    "\n",
    "# TRAIN MODEL\n",
    "print('Training model...')\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs, \n",
    "                    validation_data=(X_dev, Y_dev), callbacks=callbacks_list, verbose=verbose, shuffle=True) \n",
    "\n",
    "# SAVE MODEL AND HISTORY\n",
    "save_model_history(model, history, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE MODEL\n",
    "print('Evaluating model...')\n",
    "prices = df['current_price']\n",
    "evaluate = evaluate_model(model, history, prices, X_train, X_dev, X_test, Y_train, Y_dev, Y_test, predict_end_of_window)\n",
    "\n",
    "with open(model_path + '/evaluate.pkl', 'wb') as f:\n",
    "  pickle.dump(evaluate, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE\n",
    "## Plot: \n",
    "# Historical price, color coded with train, dev, test\n",
    "# Historical percent change, color coded with train, dev, test\n",
    "# Train Loss, Dev Loss\n",
    "# Actual price vs predicted price (or percent change) for test set\n",
    "# Example features time series for one day (NOTE: in the preprocessing_final notebook)\n",
    "\n",
    "if predict_end_of_window:\n",
    "    timestep_within_window = 0\n",
    "else:\n",
    "    timestep_within_window = window-1\n",
    "    \n",
    "minute_start = 0\n",
    "\n",
    "print_save_events_props(Y_train.flatten(), Y_dev.flatten(), Y_test.flatten(), evaluate, model_name, model_path)\n",
    "plot_price(df, X_train, X_dev, field='current_price')\n",
    "plot_price(df, X_train, X_dev, field='percent_change')\n",
    "\n",
    "plot_losses(restored_history, field='loss', title='Training Loss')\n",
    "plot_losses(restored_history, field='val_loss', title='Dev Loss')\n",
    "\n",
    "minute_end = len(Y_train)\n",
    "plot_percent_change(evaluate['y_hat_train'], Y_train, timestep_within_window, minute_start, minute_end, predict_end_of_window)\n",
    "\n",
    "minute_end = len(Y_dev)\n",
    "plot_percent_change(evaluate['y_hat_dev'], Y_dev, timestep_within_window, minute_start, minute_end, predict_end_of_window)\n",
    "\n",
    "minute_end = len(Y_test)\n",
    "plot_percent_change(evaluate['y_hat_test'], Y_test, timestep_within_window, minute_start, minute_end, predict_end_of_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('================== BASELINES ==================')\n",
    "baselines(Y_train, data_name='Training Data')\n",
    "baselines(Y_dev,   data_name='Dev Data')\n",
    "baselines(Y_test,  data_name='Test Data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
