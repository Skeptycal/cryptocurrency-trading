{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15991435804489965231\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "import lz4.frame\n",
    "import gzip\n",
    "import io\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from plumbum.cmd import rm\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import CuDNNLSTM\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotline(data):\n",
    "    plt.figure()\n",
    "    plt.plot(data)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def event_count(time_series, data_name):\n",
    "    time_series = time_series[['Fill Price (USD)']].values\n",
    "    upevents = 0\n",
    "    downevents = 0\n",
    "    sameprice = 0\n",
    "    prev_obv = time_series[0]\n",
    "    for obv in time_series[1:]:\n",
    "        if obv > prev_obv:\n",
    "            upevents += 1\n",
    "        elif obv < prev_obv:\n",
    "            downevents += 1\n",
    "        elif obv == prev_obv:\n",
    "            sameprice += 1\n",
    "        prev_obv = obv\n",
    "    print('=== Event counts on %s ===' % data_name)\n",
    "    print('upevents')\n",
    "    print(upevents)\n",
    "    print('downevents')\n",
    "    print(downevents)\n",
    "    print('sameprice')\n",
    "    print(sameprice)\n",
    "    print()\n",
    "\n",
    "def mse(time_series, data_name):\n",
    "    time_series = time_series[['Fill Price (USD)']].values\n",
    "    total_squared_error = 0\n",
    "    total_absolute_error = 0\n",
    "    prev_obv = time_series[0]\n",
    "    for obv in time_series[1:]:\n",
    "        total_squared_error += (obv - prev_obv)**2\n",
    "        total_absolute_error += abs(obv - prev_obv)\n",
    "        prev_obv = obv\n",
    "    num_predictions = len(time_series) - 1\n",
    "    mean_squared_error = total_squared_error / num_predictions\n",
    "    mean_absolute_error = total_absolute_error / num_predictions\n",
    "    root_mean_squared_error = np.sqrt(mean_squared_error)\n",
    "    print('=== baseline on %s ===' % data_name)\n",
    "    print('total squared error')\n",
    "    print(total_squared_error)\n",
    "    print('total absolute error')\n",
    "    print(total_absolute_error)\n",
    "    print('mean squared error')\n",
    "    print(mean_squared_error)\n",
    "    print('mean absolute error')\n",
    "    print(mean_absolute_error) \n",
    "    print('root mean squared error')\n",
    "    print(root_mean_squared_error) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_summary_statistics():\n",
    "    #event_count(small_set, 'small')\n",
    "    train_set = df.iloc[0:num_samples_training]\n",
    "    dev_set = df.iloc[num_samples_training:num_samples_training+num_samples_dev]\n",
    "    test_set = df.iloc[num_samples_training+num_samples_dev:]\n",
    "    event_count(train_set, 'train')\n",
    "    event_count(dev_set, 'dev')\n",
    "    event_count(test_set, 'test')\n",
    "    mse(train_set, 'train')\n",
    "    mse(dev_set, 'dev')\n",
    "    mse(test_set, 'test')\n",
    "#show_summary_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    values = np.array(data)\n",
    "    values = values.reshape(-1,1)\n",
    "    values = values.astype('float32') \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(model_history, title):\n",
    "    plt.figure()\n",
    "    plt.plot(model_history.history['loss'], label='Train')\n",
    "    plt.plot(model_history.history['val_loss'], label='Dev')\n",
    "    plt.xlabel('Epochs'); plt.ylabel('Loss (mse)')\n",
    "    plt.title(title)\n",
    "    plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform_pricescaler(data, Y_prevrawprice, fitted_scaler):\n",
    "    return fitted_scaler.inverse_transform(preprocess(data))\n",
    "\n",
    "def inverse_transform_percentdiff(data, Y_prevrawprice, fitted_scaler=None):\n",
    "    orig_prices = Y_prevrawprice\n",
    "    change = orig_prices * data\n",
    "    return orig_prices + change\n",
    "    #return fitted_scaler.inverse_transform(preprocess(data))\n",
    "\n",
    "#print(Y_test_prevrawprice)\n",
    "#print(inverse_transform_percentdiff(Y_test, Y_test_prevrawprice))\n",
    "\n",
    "inverse_transform = inverse_transform_percentdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(model, X_test, Y_test, Y_prevrawprice, title, inverse=False, scaler=None):\n",
    "    y_hat = model.predict(X_test)\n",
    "\n",
    "    if inverse:\n",
    "        y_hat = inverse_transform(y_hat, Y_prevrawprice, scaler)\n",
    "        Y_test = inverse_transform(Y_test, Y_prevrawprice, scaler)\n",
    "\n",
    "    plt.plot(y_hat, label='Predicted')\n",
    "    plt.plot(Y_test, label='True')\n",
    "    plt.xlabel('Time'); \n",
    "\n",
    "    if inverse:\n",
    "        plt.ylabel('Price')\n",
    "    else:\n",
    "        plt.ylabel('RESCALED Price')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MSE_RMSE(model, scaler, X_test, Y_test, Y_prevrawprice, model_name):\n",
    "    y_hat = model.predict(X_test)\n",
    "    y_hat_inverse = inverse_transform(y_hat, Y_prevrawprice, scaler)\n",
    "    Y_test_inverse = inverse_transform(Y_test, Y_prevrawprice, scaler)\n",
    "    mse = mean_squared_error(Y_test_inverse, y_hat_inverse)\n",
    "    rmse = np.sqrt(mean_squared_error(Y_test_inverse, y_hat_inverse))\n",
    "    print('%s:' % model_name)\n",
    "    print('Test MSE: %.3f' % mse)\n",
    "    print('Test RMSE: %.3f' % rmse)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(model, model_name, \n",
    "                   X_train, Y_train, Y_train_prevrawprice, X_dev, Y_dev, Y_dev_prevrawprice, X_test, Y_test, Y_test_prevrawprice,\n",
    "                   lag=10, batch_size=100, epochs=10, verbose=1):\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n",
    "                      validation_split=0.05, verbose=verbose, shuffle=False)\n",
    "    #train_evaluate_showresults(history, model, model_name, \n",
    "    #                 X_train, Y_train, X_dev, Y_dev, X_test, Y_test,\n",
    "    #                 lag, batch_size, epochs, verbose)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_showresults(history, model, model_name, \n",
    "                   X_train, Y_train, Y_train_prevrawprice, X_dev, Y_dev, Y_dev_prevrawprice, X_test, Y_test, Y_test_prevrawprice,\n",
    "                   lag=10, batch_size=100, epochs=10, verbose=1):\n",
    "    # Plot losses, predictions, and calculate MSE and RMSE\n",
    "    plot_losses(history, 'Loss\\n(%s)' % model_name)\n",
    "    plot_predictions(model, X_dev, Y_dev, Y_dev_prevrawprice, 'Test Predictions\\n(%s)' % model_name)\n",
    "    plot_predictions(model, X_dev, Y_dev, Y_dev_prevrawprice, 'Test Predictions\\n(%s)' % model_name, inverse=True, scaler=price_scaler)\n",
    "    calculate_MSE_RMSE(model, price_scaler, X_dev, Y_dev, Y_dev_prevrawprice, '%s' % model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(model, model_name, \n",
    "                   X_train, Y_train, Y_train_prevrawprice, X_dev, Y_dev, Y_dev_prevrawprice, X_test, Y_test, Y_test_prevrawprice,\n",
    "                   lag=10, batch_size=100, epochs=10, verbose=1):\n",
    "    # Plot losses, predictions, and calculate MSE and RMSE\n",
    "    #plot_losses(history, 'Loss\\n(%s)' % model_name)\n",
    "    plot_predictions(model, X_test, Y_test, Y_test_prevrawprice, 'Test Predictions\\n(%s)' % model_name)\n",
    "    plot_predictions(model, X_test, Y_test, Y_test_prevrawprice, 'Test Predictions\\n(%s)' % model_name, inverse=True, scaler=price_scaler)\n",
    "    calculate_MSE_RMSE(model, price_scaler, X_test, Y_test, Y_test_prevrawprice, '%s' % model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(X_train, loss, optimizer, num_LSTMs, num_units, dropout):\n",
    "    \n",
    "    LSTM_input_shape = [X_train.shape[1], X_train.shape[2]]\n",
    "\n",
    "    # DEFINE MODEL\n",
    "    model = Sequential()\n",
    "\n",
    "    if num_LSTMs == 2:\n",
    "            model.add(LSTM(num_units[0], input_shape=LSTM_input_shape, return_sequences=True))\n",
    "            model.add(Dropout(0.1))\n",
    "\n",
    "            model.add(LSTM(num_units[1], input_shape=LSTM_input_shape, return_sequences=False))\n",
    "            model.add(Dropout(0.1))\n",
    "        \n",
    "    if num_LSTMs == 3:\n",
    "            model.add(LSTM(num_units[0], input_shape=LSTM_input_shape, return_sequences=True))\n",
    "            model.add(Dropout(0.1))\n",
    "\n",
    "            model.add(LSTM(num_units[1], input_shape=LSTM_input_shape, return_sequences=True))\n",
    "            model.add(Dropout(0.1))\n",
    "            \n",
    "            model.add(LSTM(num_units[2], return_sequences=False))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    #model.add(Dense(1, kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    \n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    files = sorted(glob('cboe/parquet_preprocessed_BTCUSD/*.parquet'))\n",
    "    all_dataframes = []\n",
    "    for file in files:\n",
    "        print(file)\n",
    "        df = pq.read_table(file).to_pandas()\n",
    "        all_dataframes.append(df)\n",
    "    result = pd.concat(all_dataframes)\n",
    "    pq.write_table(pa.Table.from_pandas(result), 'cboe/parquet_preprocessed_subset_only_BTCUSD.parquet', compression='snappy')\n",
    "    df = pq.read_table('cboe/parquet_preprocessed_subset_only_BTCUSD.parquet').to_pandas();\n",
    "    print(df.dtypes)\n",
    "    print(df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_X(df):\n",
    "    n_all = df.shape[0]\n",
    "    n_train = round(n_all * 0.9)\n",
    "    n_dev   = round(n_all * 0.05)\n",
    "    n_test  = round(n_all * 0.05)\n",
    "    print('n_all:  ', n_all)\n",
    "    print('n_train:', n_train)\n",
    "    print('n_dev:  ', n_dev)\n",
    "    print('n_test: ', n_test)\n",
    "    \n",
    "    X_train = df.iloc[:n_train, 1:-1].values.astype('float32')[:, None, :]\n",
    "    X_dev   = df.iloc[n_train:n_train+n_dev, 1:-1].values.astype('float32')[:, None, :]\n",
    "    X_test  = df.iloc[n_train+n_dev:, 1:-1].values.astype('float32')[:, None, :]\n",
    "    print(X_train.shape)\n",
    "    print(X_dev.shape)\n",
    "    print(X_test.shape)\n",
    "\n",
    "    return X_train, X_dev, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_Y(df):\n",
    "    n_all = df.shape[0]\n",
    "    n_train = round(n_all * 0.9)\n",
    "    n_dev   = round(n_all * 0.05)\n",
    "    n_test  = round(n_all * 0.05)\n",
    "    Y_train = df.iloc[:n_train, -1:].values.astype('float32')\n",
    "    Y_dev   = df.iloc[n_train:n_train+n_dev, -1:].values.astype('float32')\n",
    "    Y_test  = df.iloc[n_train+n_dev:, -1:].values.astype('float32')\n",
    "    print(Y_train.shape)\n",
    "    print(Y_dev.shape)\n",
    "    print(Y_test.shape)\n",
    "    \n",
    "    return Y_train, Y_dev, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_parquet(df, outfile):\n",
    "    pq.write_table(pa.Table.from_pandas(df), outfile, compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, history, X_train, X_dev, X_test, Y_train, Y_dev, Y_test):\n",
    "    train_loss = history.history['loss'][-1]\n",
    "    dev_loss = history.history['val_loss'][-1]\n",
    "    test_loss = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    \n",
    "    y_hat_train = model.predict(X_train)\n",
    "    y_hat_dev   = model.predict(X_dev)\n",
    "    y_hat_test  = model.predict(X_test)\n",
    "    \n",
    "    train_prop_correct = np.sum(np.sign(y_hat_test) == np.sign(Y_test)) / Y_test.shape[0]\n",
    "    dev_prop_correct   = np.sum(np.sign(y_hat_dev) == np.sign(Y_dev)) / Y_dev.shape[0]\n",
    "    test_prop_correct  = np.sum(np.sign(y_hat_train) == np.sign(Y_train)) / Y_train.shape[0]\n",
    "    \n",
    "    evaluation = [train_loss, dev_loss, test_loss, train_prop_correct, dev_prop_correct, test_prop_correct]\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171002.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171003.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171004.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171005.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171006.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171007.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171008.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171009.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171010.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171011.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171012.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171013.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171014.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171015.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171016.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171017.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171018.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171019.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171020.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171021.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171022.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171023.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171024.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171025.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171026.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171027.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171028.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171029.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171030.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171031.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171101.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171102.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171103.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171104.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171105.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171106.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171107.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171108.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171109.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171110.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171111.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171112.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171113.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171114.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171115.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171116.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171117.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171118.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171119.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171120.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171121.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171122.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171123.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171124.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171125.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171126.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171127.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171128.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171129.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171130.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171201.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171202.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171203.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171204.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171205.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171206.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171207.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171208.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171209.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171210.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171211.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171212.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171213.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171214.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171215.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171216.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171217.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171218.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171219.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171220.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171221.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171222.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171223.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171224.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171225.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171226.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171227.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171228.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171229.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171230.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20171231.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180101.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180102.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180103.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180104.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180105.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180106.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180107.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180108.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180109.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180110.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180111.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180112.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180113.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180114.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180115.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180116.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180117.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180118.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180119.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180120.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180121.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180122.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180123.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180124.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180125.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180126.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180127.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180128.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180129.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180130.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180131.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180201.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180202.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180203.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180204.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180205.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180206.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180207.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180208.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180209.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180210.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180211.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180212.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180213.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180214.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180215.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180216.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180217.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180218.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180219.parquet\n",
      "cboe/parquet_preprocessed_BTCUSD/BTCUSD_order_book_20180220.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_price                           float64\n",
      "percent_change                          float64\n",
      "buy_vol_mark_vs_fillLim                 float64\n",
      "buy_vol_placeLim_vs_fillLim             float64\n",
      "buy_freq_mark_vs_fillLim                float64\n",
      "buy_freq_placeLim_vs_fillLim            float64\n",
      "sell_vol_mark_vs_fillLim                float64\n",
      "sell_vol_placeLim_vs_fillLim            float64\n",
      "sell_freq_mark_vs_fillLim               float64\n",
      "sell_freq_placeLim_vs_fillLim           float64\n",
      "vol_markSells_vs_markBuys               float64\n",
      "vol_placeLimSells_vs_placeLimBuys       float64\n",
      "vol_CancelLimSells_vs_CancelLimBuys     float64\n",
      "freq_markSells_vs_markBuys              float64\n",
      "freq_placeLimSells_vs_placeLimBuys      float64\n",
      "freq_CancelLimSells_vs_CancelLimBuys    float64\n",
      "m0                                      float64\n",
      "m1                                      float64\n",
      "m2                                      float64\n",
      "m3                                      float64\n",
      "m4                                      float64\n",
      "m5                                      float64\n",
      "m6                                      float64\n",
      "m7                                      float64\n",
      "m8                                      float64\n",
      "m9                                      float64\n",
      "m10                                     float64\n",
      "m11                                     float64\n",
      "d0                                      float64\n",
      "d1                                      float64\n",
      "d2                                      float64\n",
      "d3                                      float64\n",
      "d4                                      float64\n",
      "d5                                      float64\n",
      "d6                                      float64\n",
      "h0                                      float64\n",
      "h1                                      float64\n",
      "h2                                      float64\n",
      "h3                                      float64\n",
      "h4                                      float64\n",
      "h5                                      float64\n",
      "h6                                      float64\n",
      "h7                                      float64\n",
      "h8                                      float64\n",
      "h9                                      float64\n",
      "h10                                     float64\n",
      "h11                                     float64\n",
      "h12                                     float64\n",
      "h13                                     float64\n",
      "h14                                     float64\n",
      "h15                                     float64\n",
      "h16                                     float64\n",
      "h17                                     float64\n",
      "h18                                     float64\n",
      "h19                                     float64\n",
      "h20                                     float64\n",
      "h21                                     float64\n",
      "h22                                     float64\n",
      "h23                                     float64\n",
      "y_percent_change                        float64\n",
      "dtype: object\n",
      "(201520, 60)\n",
      "n_all:   201520\n",
      "n_train: 181368\n",
      "n_dev:   10076\n",
      "n_test:  10076\n",
      "(181368, 1, 58)\n",
      "(10076, 1, 58)\n",
      "(10076, 1, 58)\n",
      "(181368, 1)\n",
      "(10076, 1)\n",
      "(10076, 1)\n",
      "n_all:   201520\n",
      "n_train: 181368\n",
      "n_dev:   10076\n",
      "n_test:  10076\n",
      "(181368, 1, 13)\n",
      "(10076, 1, 13)\n",
      "(10076, 1, 13)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "loss = 'mse'\n",
    "optimizers = ['adagrad', 'adam', 'rmsprop']\n",
    "batch_sizes = [2048, 8192, 16384]\n",
    "include_monthDayHour = [True, False]\n",
    "num_LSTMs = [2,3]\n",
    "num_units_2 = [[128, 256], [256, 256]]\n",
    "num_units_3 = [[128, 256, 256], [256, 256, 256], [256, 512, 512]]\n",
    "#amount_of_data = {'one_year', 'all'}\n",
    "#dropout = np.random.uniform(0.5, 0.05, num_LSTMs)\n",
    "dropout = 0.1\n",
    "\n",
    "# Load data\n",
    "df = load_data()\n",
    "X_train, X_dev, X_test = split_X(df) \n",
    "Y_train, Y_dev, Y_test = split_Y(df)\n",
    "\n",
    "df_noTime = df.iloc[:,1:16]\n",
    "X_train_noTime, X_dev_noTime, X_test_noTime = split_X(df_noTime) \n",
    "\n",
    "# Initialize output dataframe\n",
    "outfile = 'cboe/grid_search_dataSubset.parquet'\n",
    "columns = ['num_epochs', 'loss', 'optimizer', 'batch_size', 'include_time', 'num_LSTMs', 'num_units',\n",
    "           'train_loss', 'dev_loss', 'test_loss', 'train_prop_correct', 'dev_prop_correct', 'test_prop_correct']\n",
    "df_output = pd.DataFrame(columns=columns)\n",
    "pq.write_table(pa.Table.from_pandas(df_output), outfile, compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 120\n",
      "Train on 181368 samples, validate on 10076 samples\n",
      "Epoch 1/10\n",
      "181368/181368 [==============================] - 16s 87us/step - loss: 0.0011 - val_loss: 6.0925e-06\n",
      "Epoch 2/10\n",
      "181368/181368 [==============================] - 12s 65us/step - loss: 1.5850e-05 - val_loss: 4.8468e-06\n",
      "Epoch 3/10\n",
      "181368/181368 [==============================] - 13s 71us/step - loss: 1.3546e-05 - val_loss: 4.4238e-06\n",
      "Epoch 4/10\n",
      "181368/181368 [==============================] - 12s 69us/step - loss: 1.2261e-05 - val_loss: 4.3835e-06\n",
      "Epoch 5/10\n",
      "181368/181368 [==============================] - 13s 73us/step - loss: 1.1193e-05 - val_loss: 4.3519e-06\n",
      "Epoch 6/10\n",
      "181368/181368 [==============================] - 12s 68us/step - loss: 1.0463e-05 - val_loss: 4.3501e-06\n",
      "Epoch 7/10\n",
      "181368/181368 [==============================] - 12s 66us/step - loss: 9.7908e-06 - val_loss: 4.3189e-06\n",
      "Epoch 8/10\n",
      "181368/181368 [==============================] - 13s 73us/step - loss: 9.2766e-06 - val_loss: 4.3038e-06\n",
      "Epoch 9/10\n",
      "181368/181368 [==============================] - 12s 67us/step - loss: 8.8080e-06 - val_loss: 4.2994e-06\n",
      "Epoch 10/10\n",
      "181368/181368 [==============================] - 12s 64us/step - loss: 8.4968e-06 - val_loss: 4.2935e-06\n",
      "10076/10076 [==============================] - 3s 312us/step\n",
      "10076/10076 [==============================] - 4s 368us/step\n",
      "10076/10076 [==============================] - 4s 420us/step\n",
      "10076/10076 [==============================] - 4s 379us/step\n",
      "10076/10076 [==============================] - 5s 501us/step\n",
      "10076/10076 [==============================] - 4s 369us/step\n",
      "10076/10076 [==============================] - 3s 296us/step\n",
      "10076/10076 [==============================] - 3s 276us/step\n",
      "10076/10076 [==============================] - 3s 296us/step\n",
      "10076/10076 [==============================] - 6s 599us/step\n",
      "10 / 120\n",
      "Train on 181368 samples, validate on 10076 samples\n",
      "Epoch 1/10\n",
      "181368/181368 [==============================] - 14s 76us/step - loss: 0.0077 - val_loss: 8.4769e-05\n",
      "Epoch 2/10\n",
      "181368/181368 [==============================] - 8s 46us/step - loss: 4.2636e-05 - val_loss: 1.5535e-05\n",
      "Epoch 3/10\n",
      "181368/181368 [==============================] - 9s 48us/step - loss: 2.9555e-05 - val_loss: 1.0495e-05\n",
      "Epoch 4/10\n",
      "181368/181368 [==============================] - 9s 48us/step - loss: 2.5259e-05 - val_loss: 8.0295e-06\n",
      "Epoch 5/10\n",
      "181368/181368 [==============================] - 9s 48us/step - loss: 2.2678e-05 - val_loss: 6.6176e-06\n",
      "Epoch 6/10\n",
      "181368/181368 [==============================] - 9s 48us/step - loss: 2.1144e-05 - val_loss: 5.8650e-06\n",
      "Epoch 7/10\n",
      "181368/181368 [==============================] - 9s 49us/step - loss: 2.0071e-05 - val_loss: 5.3971e-06\n",
      "Epoch 8/10\n",
      "181368/181368 [==============================] - 9s 48us/step - loss: 1.9197e-05 - val_loss: 5.1136e-06\n",
      "Epoch 9/10\n",
      "181368/181368 [==============================] - 9s 47us/step - loss: 1.8611e-05 - val_loss: 4.9969e-06\n",
      "Epoch 10/10\n",
      "181368/181368 [==============================] - 9s 49us/step - loss: 1.8088e-05 - val_loss: 4.8271e-06\n",
      "10076/10076 [==============================] - 3s 286us/step\n",
      "10076/10076 [==============================] - 3s 253us/step\n",
      "10076/10076 [==============================] - 3s 249us/step\n",
      "10076/10076 [==============================] - 4s 379us/step\n",
      "10076/10076 [==============================] - 6s 565us/step\n",
      "10076/10076 [==============================] - 3s 263us/step\n",
      "10076/10076 [==============================] - 3s 347us/step\n",
      "10076/10076 [==============================] - 3s 311us/step\n",
      "10076/10076 [==============================] - 4s 364us/step\n",
      "10076/10076 [==============================] - 6s 561us/step\n",
      "20 / 120\n",
      "Train on 181368 samples, validate on 10076 samples\n",
      "Epoch 1/10\n",
      "181368/181368 [==============================] - 17s 93us/step - loss: 4.1702e-05 - val_loss: 6.8766e-06\n",
      "Epoch 2/10\n",
      "181368/181368 [==============================] - 10s 54us/step - loss: 1.0431e-04 - val_loss: 1.3812e-05\n",
      "Epoch 3/10\n",
      "181368/181368 [==============================] - 10s 55us/step - loss: 4.3999e-05 - val_loss: 6.7943e-05\n",
      "Epoch 4/10\n",
      "181368/181368 [==============================] - 9s 52us/step - loss: 4.6934e-05 - val_loss: 7.7971e-05\n",
      "Epoch 5/10\n",
      "181368/181368 [==============================] - 10s 54us/step - loss: 6.7409e-05 - val_loss: 2.1253e-05\n",
      "Epoch 6/10\n",
      "181368/181368 [==============================] - 10s 55us/step - loss: 1.1799e-04 - val_loss: 2.8990e-05\n",
      "Epoch 7/10\n",
      "181368/181368 [==============================] - 10s 53us/step - loss: 8.2851e-05 - val_loss: 9.5479e-05\n",
      "Epoch 8/10\n",
      "181368/181368 [==============================] - 10s 55us/step - loss: 6.1415e-05 - val_loss: 6.1134e-06\n",
      "Epoch 9/10\n",
      "181368/181368 [==============================] - 10s 54us/step - loss: 2.3809e-05 - val_loss: 2.6279e-05\n",
      "Epoch 10/10\n",
      "181368/181368 [==============================] - 9s 51us/step - loss: 1.9321e-05 - val_loss: 2.1564e-05\n",
      "10076/10076 [==============================] - 3s 250us/step\n",
      "10076/10076 [==============================] - 3s 321us/step\n",
      "10076/10076 [==============================] - 3s 344us/step\n",
      "10076/10076 [==============================] - 4s 421us/step\n",
      "10076/10076 [==============================] - 6s 619us/step\n",
      "10076/10076 [==============================] - 3s 341us/step\n",
      "10076/10076 [==============================] - 3s 288us/step\n",
      "10076/10076 [==============================] - 3s 283us/step\n",
      "10076/10076 [==============================] - 4s 383us/step\n",
      "10076/10076 [==============================] - 6s 593us/step\n",
      "30 / 120\n",
      "Train on 181368 samples, validate on 10076 samples\n",
      "Epoch 1/10\n",
      "181368/181368 [==============================] - 20s 112us/step - loss: 1.2488e-04 - val_loss: 1.3234e-04\n",
      "Epoch 2/10\n",
      "181368/181368 [==============================] - 13s 69us/step - loss: 2.7931e-05 - val_loss: 5.7340e-05\n",
      "Epoch 3/10\n",
      "181368/181368 [==============================] - 12s 68us/step - loss: 1.6481e-05 - val_loss: 9.5718e-06\n",
      "Epoch 4/10\n",
      "181368/181368 [==============================] - 12s 65us/step - loss: 1.0640e-05 - val_loss: 6.5581e-06\n",
      "Epoch 5/10\n",
      "181368/181368 [==============================] - 13s 69us/step - loss: 7.4010e-06 - val_loss: 5.5954e-06\n",
      "Epoch 6/10\n",
      "181368/181368 [==============================] - 13s 70us/step - loss: 5.7146e-06 - val_loss: 4.9532e-06\n",
      "Epoch 7/10\n",
      "181368/181368 [==============================] - 12s 66us/step - loss: 4.9793e-06 - val_loss: 4.5976e-06\n",
      "Epoch 8/10\n",
      "181368/181368 [==============================] - 13s 70us/step - loss: 4.6983e-06 - val_loss: 5.4599e-06\n",
      "Epoch 9/10\n",
      "181368/181368 [==============================] - 12s 66us/step - loss: 4.6114e-06 - val_loss: 4.4505e-06\n",
      "Epoch 10/10\n",
      "181368/181368 [==============================] - 12s 68us/step - loss: 4.5842e-06 - val_loss: 5.4850e-06\n",
      "10076/10076 [==============================] - 3s 333us/step\n",
      "10076/10076 [==============================] - 3s 317us/step\n",
      "10076/10076 [==============================] - 4s 396us/step\n",
      "10076/10076 [==============================] - 3s 312us/step\n",
      "10076/10076 [==============================] - 6s 589us/step\n",
      "10076/10076 [==============================] - 2s 218us/step\n",
      "10076/10076 [==============================] - 3s 329us/step\n",
      "10076/10076 [==============================] - 5s 450us/step\n",
      "10076/10076 [==============================] - 4s 388us/step\n",
      "10076/10076 [==============================] - 6s 612us/step\n",
      "40 / 120\n",
      "Train on 181368 samples, validate on 10076 samples\n",
      "Epoch 1/10\n",
      "181368/181368 [==============================] - 19s 103us/step - loss: 6.6586e-04 - val_loss: 4.8553e-05\n",
      "Epoch 2/10\n",
      "181368/181368 [==============================] - 9s 48us/step - loss: 5.2797e-05 - val_loss: 2.4474e-04\n",
      "Epoch 3/10\n",
      "181368/181368 [==============================] - 9s 48us/step - loss: 7.9235e-05 - val_loss: 3.8311e-05\n",
      "Epoch 4/10\n",
      "181368/181368 [==============================] - 9s 47us/step - loss: 9.3518e-05 - val_loss: 6.4965e-05\n",
      "Epoch 5/10\n",
      "181368/181368 [==============================] - 9s 48us/step - loss: 4.5316e-05 - val_loss: 6.9106e-06\n",
      "Epoch 6/10\n",
      "181368/181368 [==============================] - 9s 48us/step - loss: 7.3393e-05 - val_loss: 1.6345e-05\n",
      "Epoch 7/10\n",
      "181368/181368 [==============================] - 9s 49us/step - loss: 1.2005e-05 - val_loss: 2.7576e-05\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181368/181368 [==============================] - 9s 48us/step - loss: 5.8487e-05 - val_loss: 1.3309e-05\n",
      "Epoch 9/10\n",
      "181368/181368 [==============================] - 9s 48us/step - loss: 3.5199e-05 - val_loss: 3.5619e-05\n",
      "Epoch 10/10\n",
      "181368/181368 [==============================] - 9s 50us/step - loss: 1.7352e-05 - val_loss: 9.0178e-06\n",
      "10076/10076 [==============================] - 3s 284us/step\n",
      "10076/10076 [==============================] - 4s 375us/step\n",
      "10076/10076 [==============================] - 5s 451us/step\n",
      "10076/10076 [==============================] - 3s 338us/step\n",
      "10076/10076 [==============================] - 8s 777us/step\n",
      "10076/10076 [==============================] - 4s 364us/step\n",
      "10076/10076 [==============================] - 3s 266us/step\n",
      "10076/10076 [==============================] - 3s 287us/step\n",
      "10076/10076 [==============================] - 4s 402us/step\n",
      "10076/10076 [==============================] - 5s 490us/step\n",
      "50 / 120\n",
      "Train on 181368 samples, validate on 10076 samples\n",
      "Epoch 1/10\n",
      "181368/181368 [==============================] - 21s 118us/step - loss: 0.0029 - val_loss: 5.7667e-06\n",
      "Epoch 2/10\n",
      "181368/181368 [==============================] - 9s 52us/step - loss: 1.2855e-05 - val_loss: 5.0029e-06\n",
      "Epoch 3/10\n",
      "181368/181368 [==============================] - 10s 54us/step - loss: 1.2044e-05 - val_loss: 4.7437e-06\n",
      "Epoch 4/10\n",
      "181368/181368 [==============================] - 10s 55us/step - loss: 1.1395e-05 - val_loss: 4.6370e-06\n",
      "Epoch 5/10\n",
      "181368/181368 [==============================] - 10s 56us/step - loss: 1.1052e-05 - val_loss: 4.5685e-06\n",
      "Epoch 6/10\n",
      "181368/181368 [==============================] - 10s 55us/step - loss: 1.0718e-05 - val_loss: 4.5334e-06\n",
      "Epoch 7/10\n",
      "181368/181368 [==============================] - 10s 56us/step - loss: 1.0431e-05 - val_loss: 4.4882e-06\n",
      "Epoch 8/10\n",
      "181368/181368 [==============================] - 10s 55us/step - loss: 1.0118e-05 - val_loss: 4.4692e-06\n",
      "Epoch 9/10\n",
      "181368/181368 [==============================] - 10s 56us/step - loss: 9.8654e-06 - val_loss: 4.4513e-06\n",
      "Epoch 10/10\n",
      "181368/181368 [==============================] - 10s 55us/step - loss: 9.6869e-06 - val_loss: 4.4217e-06\n",
      "10076/10076 [==============================] - 3s 257us/step\n",
      "10076/10076 [==============================] - 4s 348us/step\n",
      "10076/10076 [==============================] - 4s 433us/step\n",
      "10076/10076 [==============================] - 5s 526us/step\n",
      "10076/10076 [==============================] - 7s 736us/step\n",
      "10076/10076 [==============================] - 3s 310us/step\n",
      "10076/10076 [==============================] - 3s 321us/step\n",
      "10076/10076 [==============================] - 5s 485us/step\n",
      "10076/10076 [==============================] - 4s 422us/step\n",
      "10076/10076 [==============================] - 6s 617us/step\n",
      "60 / 120\n",
      "Train on 181368 samples, validate on 10076 samples\n",
      "Epoch 1/10\n",
      "181368/181368 [==============================] - 25s 140us/step - loss: 1.9750e-05 - val_loss: 4.3324e-06\n",
      "Epoch 2/10\n",
      "181368/181368 [==============================] - 12s 67us/step - loss: 5.4980e-06 - val_loss: 4.2524e-06\n",
      "Epoch 3/10\n",
      "181368/181368 [==============================] - 12s 67us/step - loss: 4.9503e-06 - val_loss: 4.2541e-06\n",
      "Epoch 4/10\n",
      "181368/181368 [==============================] - 13s 73us/step - loss: 4.7377e-06 - val_loss: 4.3158e-06\n",
      "Epoch 5/10\n",
      "181368/181368 [==============================] - 12s 68us/step - loss: 4.6414e-06 - val_loss: 4.2891e-06\n",
      "Epoch 6/10\n",
      "181368/181368 [==============================] - 12s 67us/step - loss: 4.5835e-06 - val_loss: 4.2523e-06\n",
      "Epoch 7/10\n",
      "181368/181368 [==============================] - 12s 69us/step - loss: 4.5303e-06 - val_loss: 4.2626e-06\n",
      "Epoch 8/10\n",
      "181368/181368 [==============================] - 13s 72us/step - loss: 4.4939e-06 - val_loss: 4.2517e-06\n",
      "Epoch 9/10\n",
      "181368/181368 [==============================] - 12s 69us/step - loss: 4.4743e-06 - val_loss: 4.2544e-06\n",
      "Epoch 10/10\n",
      "181368/181368 [==============================] - 12s 68us/step - loss: 4.4518e-06 - val_loss: 4.2482e-06\n",
      "10076/10076 [==============================] - 2s 236us/step\n",
      "10076/10076 [==============================] - 3s 269us/step\n",
      "10076/10076 [==============================] - 4s 432us/step\n",
      "10076/10076 [==============================] - 4s 402us/step\n",
      "10076/10076 [==============================] - 7s 709us/step\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for include_time in include_monthDayHour:\n",
    "    if not include_time:\n",
    "        X_train = X_train_noTime\n",
    "        X_dev =   X_dev_noTime\n",
    "        X_test =  X_test_noTime\n",
    "\n",
    "    for optimizer in optimizers:\n",
    "        for batch_size in batch_sizes:\n",
    "            for num_LSTM in num_LSTMs:\n",
    "                if num_LSTM == 2:\n",
    "                    num_units = num_units_2\n",
    "                elif num_LSTM == 3:\n",
    "                    num_units = num_units_3\n",
    "                for n_units in num_units:\n",
    "                    # Load output dataframe\n",
    "                    df_output = pq.read_table(outfile).to_pandas()\n",
    "                    \n",
    "                    # Initialize model\n",
    "                    model = initialize_model(X_train, loss, optimizer, num_LSTM, n_units, dropout)\n",
    "                    \n",
    "                    # Train model\n",
    "                    if count%10==0:\n",
    "                        verbose=1\n",
    "                        print(count, '/', 120)\n",
    "                    else:\n",
    "                        verbose=0\n",
    "                    history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n",
    "                      validation_data=(X_dev, Y_dev), verbose=verbose, shuffle=False) \n",
    "                    \n",
    "                    # Evaluate model\n",
    "                    evaluate = evaluate_model(model, history, X_train, X_dev, X_test, Y_train, Y_dev, Y_test)\n",
    "                    \n",
    "                    # Write to dataframe and save\n",
    "                    row = [num_epochs, loss, optimizer, batch_size, include_time, num_LSTM, str(n_units)]\n",
    "                    row.extend(evaluate)\n",
    "                    df_output.loc[len(df_output)] = row\n",
    "                    df_to_parquet(df_output, outfile)\n",
    "                    \n",
    "                    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 164992 samples, validate on 9166 samples\n",
      "Epoch 1/10\n",
      "164992/164992 [==============================] - 44s 265us/step - loss: 0.0012 - val_loss: 9.1829e-06\n",
      "Epoch 2/10\n",
      "164992/164992 [==============================] - 40s 245us/step - loss: 7.7286e-06 - val_loss: 4.1735e-06\n",
      "Epoch 3/10\n",
      "164992/164992 [==============================] - 41s 249us/step - loss: 5.7605e-06 - val_loss: 3.4133e-06\n",
      "Epoch 4/10\n",
      "114688/164992 [===================>..........] - ETA: 11s - loss: 4.4472e-06"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-b4f48f8bdd55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_LSTMs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n\u001b[0;32m---> 13\u001b[0;31m                       validation_data=(X_dev, Y_dev), verbose=1, shuffle=False)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2048 batch, 3 cells, numunits[2]\n",
    "# HYPERPARAMETERS\n",
    "#####################\n",
    "loss = 'mean_squared_error'\n",
    "optimizer = 'adagrad'\n",
    "batch_size = 2048\n",
    "epochs = 10\n",
    "\n",
    "num_LSTMs = 3\n",
    "num_units = num_units_3[2]\n",
    "model = initialize_model(X_train, loss, optimizer, num_LSTMs, num_units, dropout)\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n",
    "                      validation_data=(X_dev, Y_dev), verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
