{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "import lz4.frame\n",
    "import gzip\n",
    "import io\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from plumbum.cmd import rm\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())\n",
    "\n",
    "#from keras import backend as K\n",
    "#print(K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotline(data):\n",
    "    plt.figure()\n",
    "    plt.plot(data)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def event_count(time_series, data_name):\n",
    "    time_series = time_series[['Fill Price (USD)']].values\n",
    "    upevents = 0\n",
    "    downevents = 0\n",
    "    sameprice = 0\n",
    "    prev_obv = time_series[0]\n",
    "    for obv in time_series[1:]:\n",
    "        if obv > prev_obv:\n",
    "            upevents += 1\n",
    "        elif obv < prev_obv:\n",
    "            downevents += 1\n",
    "        elif obv == prev_obv:\n",
    "            sameprice += 1\n",
    "        prev_obv = obv\n",
    "    print('=== Event counts on %s ===' % data_name)\n",
    "    print('upevents')\n",
    "    print(upevents)\n",
    "    print('downevents')\n",
    "    print(downevents)\n",
    "    print('sameprice')\n",
    "    print(sameprice)\n",
    "    print()\n",
    "\n",
    "def mse(time_series, data_name):\n",
    "    time_series = time_series[['Fill Price (USD)']].values\n",
    "    total_squared_error = 0\n",
    "    total_absolute_error = 0\n",
    "    prev_obv = time_series[0]\n",
    "    for obv in time_series[1:]:\n",
    "        total_squared_error += (obv - prev_obv)**2\n",
    "        total_absolute_error += abs(obv - prev_obv)\n",
    "        prev_obv = obv\n",
    "    num_predictions = len(time_series) - 1\n",
    "    mean_squared_error = total_squared_error / num_predictions\n",
    "    mean_absolute_error = total_absolute_error / num_predictions\n",
    "    root_mean_squared_error = np.sqrt(mean_squared_error)\n",
    "    print('=== baseline on %s ===' % data_name)\n",
    "    print('total squared error')\n",
    "    print(total_squared_error)\n",
    "    print('total absolute error')\n",
    "    print(total_absolute_error)\n",
    "    print('mean squared error')\n",
    "    print(mean_squared_error)\n",
    "    print('mean absolute error')\n",
    "    print(mean_absolute_error) \n",
    "    print('root mean squared error')\n",
    "    print(root_mean_squared_error) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_summary_statistics():\n",
    "    #event_count(small_set, 'small')\n",
    "    train_set = df.iloc[0:num_samples_training]\n",
    "    dev_set = df.iloc[num_samples_training:num_samples_training+num_samples_dev]\n",
    "    test_set = df.iloc[num_samples_training+num_samples_dev:]\n",
    "    event_count(train_set, 'train')\n",
    "    event_count(dev_set, 'dev')\n",
    "    event_count(test_set, 'test')\n",
    "    mse(train_set, 'train')\n",
    "    mse(dev_set, 'dev')\n",
    "    mse(test_set, 'test')\n",
    "#show_summary_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    values = np.array(data)\n",
    "    values = values.reshape(-1,1)\n",
    "    values = values.astype('float32') \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(model_history, title):\n",
    "    plt.figure()\n",
    "    plt.plot(model_history.history['loss'], label='Train')\n",
    "    plt.plot(model_history.history['val_loss'], label='Dev')\n",
    "    plt.xlabel('Epochs'); plt.ylabel('Loss (mse)')\n",
    "    plt.title(title)\n",
    "    plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform_pricescaler(data, Y_prevrawprice, fitted_scaler):\n",
    "    return fitted_scaler.inverse_transform(preprocess(data))\n",
    "\n",
    "def inverse_transform_percentdiff(data, Y_prevrawprice, fitted_scaler=None):\n",
    "    orig_prices = Y_prevrawprice\n",
    "    change = orig_prices * data\n",
    "    return orig_prices + change\n",
    "    #return fitted_scaler.inverse_transform(preprocess(data))\n",
    "\n",
    "#print(Y_test_prevrawprice)\n",
    "#print(inverse_transform_percentdiff(Y_test, Y_test_prevrawprice))\n",
    "\n",
    "inverse_transform = inverse_transform_percentdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(model, X_test, Y_test, Y_prevrawprice, title, inverse=False, scaler=None):\n",
    "    y_hat = model.predict(X_test)\n",
    "\n",
    "    if inverse:\n",
    "        y_hat = inverse_transform(y_hat, Y_prevrawprice, scaler)\n",
    "        Y_test = inverse_transform(Y_test, Y_prevrawprice, scaler)\n",
    "\n",
    "    plt.plot(y_hat, label='Predicted')\n",
    "    plt.plot(Y_test, label='True')\n",
    "    plt.xlabel('Time'); \n",
    "\n",
    "    if inverse:\n",
    "        plt.ylabel('Price')\n",
    "    else:\n",
    "        plt.ylabel('RESCALED Price')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MSE_RMSE(model, scaler, X_test, Y_test, Y_prevrawprice, model_name):\n",
    "    y_hat = model.predict(X_test)\n",
    "    y_hat_inverse = inverse_transform(y_hat, Y_prevrawprice, scaler)\n",
    "    Y_test_inverse = inverse_transform(Y_test, Y_prevrawprice, scaler)\n",
    "    mse = mean_squared_error(Y_test_inverse, y_hat_inverse)\n",
    "    rmse = np.sqrt(mean_squared_error(Y_test_inverse, y_hat_inverse))\n",
    "    print('%s:' % model_name)\n",
    "    print('Test MSE: %.3f' % mse)\n",
    "    print('Test RMSE: %.3f' % rmse)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(model, model_name, \n",
    "                   X_train, Y_train, Y_train_prevrawprice, X_dev, Y_dev, Y_dev_prevrawprice, X_test, Y_test, Y_test_prevrawprice,\n",
    "                   lag=10, batch_size=100, epochs=10, verbose=1):\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n",
    "                      validation_split=0.05, verbose=verbose, shuffle=False)\n",
    "    #train_evaluate_showresults(history, model, model_name, \n",
    "    #                 X_train, Y_train, X_dev, Y_dev, X_test, Y_test,\n",
    "    #                 lag, batch_size, epochs, verbose)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_showresults(history, model, model_name, \n",
    "                   X_train, Y_train, Y_train_prevrawprice, X_dev, Y_dev, Y_dev_prevrawprice, X_test, Y_test, Y_test_prevrawprice,\n",
    "                   lag=10, batch_size=100, epochs=10, verbose=1):\n",
    "    # Plot losses, predictions, and calculate MSE and RMSE\n",
    "    plot_losses(history, 'Loss\\n(%s)' % model_name)\n",
    "    plot_predictions(model, X_dev, Y_dev, Y_dev_prevrawprice, 'Test Predictions\\n(%s)' % model_name)\n",
    "    plot_predictions(model, X_dev, Y_dev, Y_dev_prevrawprice, 'Test Predictions\\n(%s)' % model_name, inverse=True, scaler=price_scaler)\n",
    "    calculate_MSE_RMSE(model, price_scaler, X_dev, Y_dev, Y_dev_prevrawprice, '%s' % model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(model, model_name, \n",
    "                   X_train, Y_train, Y_train_prevrawprice, X_dev, Y_dev, Y_dev_prevrawprice, X_test, Y_test, Y_test_prevrawprice,\n",
    "                   lag=10, batch_size=100, epochs=10, verbose=1):\n",
    "    # Plot losses, predictions, and calculate MSE and RMSE\n",
    "    #plot_losses(history, 'Loss\\n(%s)' % model_name)\n",
    "    plot_predictions(model, X_test, Y_test, Y_test_prevrawprice, 'Test Predictions\\n(%s)' % model_name)\n",
    "    plot_predictions(model, X_test, Y_test, Y_test_prevrawprice, 'Test Predictions\\n(%s)' % model_name, inverse=True, scaler=price_scaler)\n",
    "    calculate_MSE_RMSE(model, price_scaler, X_test, Y_test, Y_test_prevrawprice, '%s' % model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef initialize_model(X_train, loss, optimizer, num_LSTMs, num_units, dropout):\\n    \\n    LSTM_input_shape = [X_train.shape[1], X_train.shape[2]]\\n\\n    # DEFINE MODEL\\n    model = Sequential()\\n\\n    if num_LSTMs == 2:\\n            model.add(LSTM(num_units[0], input_shape=LSTM_input_shape, return_sequences=True))\\n            model.add(Dropout(dropout))\\n\\n            model.add(LSTM(num_units[1], input_shape=LSTM_input_shape, return_sequences=True))\\n            model.add(Dropout(dropout))\\n        \\n    if num_LSTMs == 3:\\n            model.add(LSTM(num_units[0], input_shape=LSTM_input_shape, return_sequences=True))\\n            model.add(Dropout(dropout))\\n\\n            model.add(LSTM(num_units[1], input_shape=LSTM_input_shape, return_sequences=True))\\n            model.add(Dropout(dropout))\\n            \\n            model.add(LSTM(num_units[2], return_sequences=False))\\n\\n    #model.add(Dense(1))\\n    model.add(TimeDistributed(Dense(1), input_shape=(LSTM_input_shape)))\\n    #model.add(Dense(1, kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\\n    model.add(Activation('linear'))\\n\\n    \\n    model.compile(loss=loss, optimizer=optimizer)\\n    \\n    return model\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def initialize_model(X_train, loss, optimizer, num_LSTMs, num_units, dropout):\n",
    "    \n",
    "    LSTM_input_shape = [X_train.shape[1], X_train.shape[2]]\n",
    "\n",
    "    # DEFINE MODEL\n",
    "    model = Sequential()\n",
    "\n",
    "    if num_LSTMs == 2:\n",
    "            model.add(LSTM(num_units[0], input_shape=LSTM_input_shape, return_sequences=True))\n",
    "            model.add(Dropout(dropout))\n",
    "\n",
    "            model.add(LSTM(num_units[1], input_shape=LSTM_input_shape, return_sequences=True))\n",
    "            model.add(Dropout(dropout))\n",
    "        \n",
    "    if num_LSTMs == 3:\n",
    "            model.add(LSTM(num_units[0], input_shape=LSTM_input_shape, return_sequences=True))\n",
    "            model.add(Dropout(dropout))\n",
    "\n",
    "            model.add(LSTM(num_units[1], input_shape=LSTM_input_shape, return_sequences=True))\n",
    "            model.add(Dropout(dropout))\n",
    "            \n",
    "            model.add(LSTM(num_units[2], return_sequences=False))\n",
    "\n",
    "    #model.add(Dense(1))\n",
    "    model.add(TimeDistributed(Dense(1), input_shape=(LSTM_input_shape)))\n",
    "    #model.add(Dense(1, kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    \n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    \n",
    "    return model\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geza\n",
    "def initialize_model(X_train, loss, optimizer, num_LSTMs, num_units, dropout):\n",
    "    \n",
    "    #LSTM_input_shape = [X_train.shape[1], X_train.shape[2]]\n",
    "    LSTM_input_shape = [X_train.shape[1], X_train.shape[2]]\n",
    "    print('input shape is')\n",
    "    print(LSTM_input_shape)\n",
    "\n",
    "    # DEFINE MODEL\n",
    "    model = Sequential()\n",
    "\n",
    "    if num_LSTMs == 2:\n",
    "            model.add(LSTM(num_units[0], input_shape=LSTM_input_shape, return_sequences=True))\n",
    "            model.add(Dropout(dropout))\n",
    "\n",
    "            model.add(LSTM(num_units[1], return_sequences=True))\n",
    "        \n",
    "    if num_LSTMs == 3:\n",
    "            model.add(LSTM(num_units[0], input_shape=LSTM_input_shape, return_sequences=True))\n",
    "            model.add(Dropout(dropout))\n",
    "\n",
    "            model.add(LSTM(num_units[1], return_sequences=True))\n",
    "            model.add(Dropout(dropout))\n",
    "            \n",
    "            model.add(LSTM(num_units[2], return_sequences=True))\n",
    "\n",
    "    #model.add(Dense(1))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    #model.add(Dense(1, kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    \n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef load_data():\\n    files = sorted(glob('cboe/parquet_preprocessed_BTCUSD/*.parquet'))\\n    all_dataframes = []\\n    for file in files:\\n        print(file)\\n        df = pq.read_table(file).to_pandas()\\n        all_dataframes.append(df)\\n    result = pd.concat(all_dataframes)\\n    pq.write_table(pa.Table.from_pandas(result), 'cboe/parquet_preprocessed_subset_only_BTCUSD.parquet', compression='snappy')\\n    df = pq.read_table('cboe/parquet_preprocessed_subset_only_BTCUSD.parquet').to_pandas();\\n    print(df.dtypes)\\n    print(df.shape)\\n    return df\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def load_data():\n",
    "    files = sorted(glob('cboe/parquet_preprocessed_BTCUSD/*.parquet'))\n",
    "    all_dataframes = []\n",
    "    for file in files:\n",
    "        print(file)\n",
    "        df = pq.read_table(file).to_pandas()\n",
    "        all_dataframes.append(df)\n",
    "    result = pd.concat(all_dataframes)\n",
    "    pq.write_table(pa.Table.from_pandas(result), 'cboe/parquet_preprocessed_subset_only_BTCUSD.parquet', compression='snappy')\n",
    "    df = pq.read_table('cboe/parquet_preprocessed_subset_only_BTCUSD.parquet').to_pandas();\n",
    "    print(df.dtypes)\n",
    "    print(df.shape)\n",
    "    return df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geza\n",
    "import os.path\n",
    "\n",
    "def load_data():\n",
    "    if not os.path.isfile('cboe/parquet_preprocessed_subset_only_BTCUSD_merged.parquet'):\n",
    "        files = sorted(glob('cboe/parquet_preprocessed_BTCUSD_merged/*.parquet'))\n",
    "        all_dataframes = []\n",
    "        for file in files:\n",
    "            print(file)\n",
    "            df = pq.read_table(file).to_pandas()\n",
    "            all_dataframes.append(df)\n",
    "        result = pd.concat(all_dataframes)\n",
    "        pq.write_table(pa.Table.from_pandas(result), 'cboe/parquet_preprocessed_subset_only_BTCUSD_merged.parquet', compression='snappy')\n",
    "    df = pq.read_table('cboe/parquet_preprocessed_subset_only_BTCUSD_merged.parquet').to_pandas();\n",
    "    print(df.dtypes)\n",
    "    print(df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_X(df):\n",
    "    n_all = df.shape[0]\n",
    "    n_train = round(n_all * 0.9)\n",
    "    n_dev   = round(n_all * 0.05)\n",
    "    n_test  = round(n_all * 0.05)\n",
    "    print('n_all:  ', n_all)\n",
    "    print('n_train:', n_train)\n",
    "    print('n_dev:  ', n_dev)\n",
    "    print('n_test: ', n_test)\n",
    "    \n",
    "#     X_train = df.iloc[:n_train, 1:-1].values.astype('float32')[:, None, :]\n",
    "#     X_dev   = df.iloc[n_train:n_train+n_dev, 1:-1].values.astype('float32')[:, None, :]\n",
    "#     X_test  = df.iloc[n_train+n_dev:, 1:-1].values.astype('float32')[:, None, :]\n",
    "#     X_train = df.iloc[:n_train, 1:-1].values.astype('float32')\n",
    "#     X_dev   = df.iloc[n_train:n_train+n_dev, 1:-1].values.astype('float32')\n",
    "#     X_test  = df.iloc[n_train+n_dev:, 1:-1].values.astype('float32')\n",
    "    X_train = df.iloc[:n_train, 1:16].values.astype('float32')\n",
    "    X_dev   = df.iloc[n_train:n_train+n_dev, 1:16].values.astype('float32')\n",
    "    X_test  = df.iloc[n_train+n_dev:, 1:16].values.astype('float32')\n",
    "    print(X_train.shape)\n",
    "    print(X_dev.shape)\n",
    "    print(X_test.shape)\n",
    "\n",
    "    return X_train, X_dev, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_Y(df):\n",
    "    n_all = df.shape[0]\n",
    "    n_train = round(n_all * 0.9)\n",
    "    n_dev   = round(n_all * 0.05)\n",
    "    n_test  = round(n_all * 0.05)\n",
    "    Y_train = df.iloc[:n_train, -1:].values.astype('float32')\n",
    "    Y_dev   = df.iloc[n_train:n_train+n_dev, -1:].values.astype('float32')\n",
    "    Y_test  = df.iloc[n_train+n_dev:, -1:].values.astype('float32')\n",
    "    print(Y_train.shape)\n",
    "    print(Y_dev.shape)\n",
    "    print(Y_test.shape)\n",
    "    \n",
    "    return Y_train, Y_dev, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_parquet(df, outfile):\n",
    "    pq.write_table(pa.Table.from_pandas(df), outfile, compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, history, X_train, X_dev, X_test, Y_train, Y_dev, Y_test):\n",
    "    train_loss = history.history['loss'][-1]\n",
    "    dev_loss = history.history['val_loss'][-1]\n",
    "    test_loss = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    \n",
    "    y_hat_train = model.predict(X_train)\n",
    "    y_hat_dev   = model.predict(X_dev)\n",
    "    y_hat_test  = model.predict(X_test)\n",
    "    \n",
    "    train_prop_correct = np.sum(np.sign(y_hat_train) == np.sign(Y_train)) / (Y_train_final.shape[0] * Y_train_final.shape[1])\n",
    "    dev_prop_correct   = np.sum(np.sign(y_hat_dev)   == np.sign(Y_dev))   / (Y_dev_final.shape[0]   * Y_dev_final.shape[1])\n",
    "    test_prop_correct  = np.sum(np.sign(y_hat_test)  == np.sign(Y_test))  / (Y_test_final.shape[0]  * Y_test_final.shape[1])\n",
    "    \n",
    "    evaluation = [train_loss, dev_loss, test_loss, train_prop_correct, dev_prop_correct, test_prop_correct]\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = result\n",
    "# n_all = df.shape[0]\n",
    "# n_train = round(n_all * 0.9)\n",
    "# n_dev   = round(n_all * 0.05)\n",
    "# n_test  = round(n_all * 0.05)\n",
    "# print('n_all:  ', n_all)\n",
    "# print('n_train:', n_train)\n",
    "# print('n_dev:  ', n_dev)\n",
    "# print('n_test: ', n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequenced_data(data, window, step, y=True):\n",
    "    sequenced = []\n",
    "    for minute in range(0, len(data) - window, step):\n",
    "        chunk = data[minute:minute+window]\n",
    "        sequenced.append(chunk)\n",
    "    sequenced = np.array(sequenced)\n",
    "#     if y:\n",
    "#         sequenced = np.reshape(sequenced, [len(sequenced), window])  \n",
    "    return sequenced\n",
    "\n",
    "# def create_sequenced_data_Y(Y, window, step):\n",
    "#     sequenced = []\n",
    "#     for minute in range(0, len(data) - window, step):\n",
    "#         chunk = data[minute:minute+window]\n",
    "#         sequenced.append(chunk)\n",
    "#     Y_train_final = np.reshape(Y_train_final, [len(Y_train_final), 60]).shape\n",
    "#     return np.array(sequenced)\n",
    "\n",
    "\n",
    "\n",
    "# Y_train = df.iloc[:len(df), -1:].values.astype('float32')\n",
    "# Y_train_final = create_sequenced_data(Y_train, 60, 1)\n",
    "# Y_train_final = np.reshape(Y_train_final, [len(Y_train_final), 60]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(glob('cboe/parquet_preprocessed_BTCUSD_merged/*.parquet'))\n",
    "all_dataframes = []\n",
    "for file in files:\n",
    "    df = pq.read_table(file).to_pandas()\n",
    "    all_dataframes.append(df)\n",
    "df = pd.concat(all_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nX_train, X_dev, X_test = split_X(df)\\nY_train, Y_dev, Y_test = split_Y(df)\\n\\nX_train_final = create_sequenced_data(X_train, window=60, step=1, y=False)\\nX_dev_final   = create_sequenced_data(X_dev, window=60, step=1, y=False)\\nX_test_final  = create_sequenced_data(X_test, window=60, step=1, y=False)\\n\\nY_train_final = create_sequenced_data(Y_train, window=60, step=1, y=True)\\nY_dev_final   = create_sequenced_data(Y_dev, window=60, step=1, y=True)\\nY_test_final  = create_sequenced_data(Y_test, window=60, step=1, y=True)\\n\\nprint('Train, dev, test shapes:')\\nprint(X_train_final.shape)\\nprint(X_dev_final.shape)\\nprint(X_test_final.shape)\\nprint(Y_train_final.shape)\\nprint(Y_dev_final.shape)\\nprint(Y_test_final.shape)\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "X_train, X_dev, X_test = split_X(df)\n",
    "Y_train, Y_dev, Y_test = split_Y(df)\n",
    "\n",
    "X_train_final = create_sequenced_data(X_train, window=60, step=1, y=False)\n",
    "X_dev_final   = create_sequenced_data(X_dev, window=60, step=1, y=False)\n",
    "X_test_final  = create_sequenced_data(X_test, window=60, step=1, y=False)\n",
    "\n",
    "Y_train_final = create_sequenced_data(Y_train, window=60, step=1, y=True)\n",
    "Y_dev_final   = create_sequenced_data(Y_dev, window=60, step=1, y=True)\n",
    "Y_test_final  = create_sequenced_data(Y_test, window=60, step=1, y=True)\n",
    "\n",
    "print('Train, dev, test shapes:')\n",
    "print(X_train_final.shape)\n",
    "print(X_dev_final.shape)\n",
    "print(X_test_final.shape)\n",
    "print(Y_train_final.shape)\n",
    "print(Y_dev_final.shape)\n",
    "print(Y_test_final.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_all:   1222293\n",
      "n_train: 1100064\n",
      "n_dev:   61115\n",
      "n_test:  61115\n",
      "(1100064, 15)\n",
      "(61115, 15)\n",
      "(61114, 15)\n",
      "(1100064, 1)\n",
      "(61115, 1)\n",
      "(61114, 1)\n",
      "Train, dev, test shapes:\n",
      "(1100044, 20, 15)\n",
      "(61095, 20, 15)\n",
      "(61094, 20, 15)\n",
      "(1100044, 20, 1)\n",
      "(61095, 20, 1)\n",
      "(61094, 20, 1)\n",
      "Train, dev, test shapes original:\n",
      "(1100064, 15)\n"
     ]
    }
   ],
   "source": [
    "#geza\n",
    "X_train, X_dev, X_test = split_X(df)\n",
    "Y_train, Y_dev, Y_test = split_Y(df)\n",
    "\n",
    "window_size = 60\n",
    "\n",
    "X_train_final = create_sequenced_data(X_train, window=window_size, step=1, y=False)\n",
    "X_dev_final   = create_sequenced_data(X_dev, window=window_size, step=1, y=False)\n",
    "X_test_final  = create_sequenced_data(X_test, window=window_size, step=1, y=False)\n",
    "\n",
    "Y_train_final = create_sequenced_data(Y_train, window=window_size, step=1, y=True)\n",
    "Y_dev_final   = create_sequenced_data(Y_dev, window=window_size, step=1, y=True)\n",
    "Y_test_final  = create_sequenced_data(Y_test, window=window_size, step=1, y=True)\n",
    "\n",
    "print('Train, dev, test shapes:')\n",
    "print(X_train_final.shape)\n",
    "print(X_dev_final.shape)\n",
    "print(X_test_final.shape)\n",
    "print(Y_train_final.shape)\n",
    "print(Y_dev_final.shape)\n",
    "print(Y_test_final.shape)\n",
    "\n",
    "print('Train, dev, test shapes original:')\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape is\n",
      "[20, 15]\n",
      "Train on 1100044 samples, validate on 61095 samples\n",
      "Epoch 1/200\n",
      " - 72s - loss: 0.3484 - val_loss: 0.0028\n",
      "Epoch 2/200\n",
      " - 67s - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 3/200\n",
      " - 67s - loss: 0.0011 - val_loss: 6.8551e-04\n",
      "Epoch 4/200\n",
      " - 67s - loss: 7.6866e-04 - val_loss: 4.6540e-04\n",
      "Epoch 5/200\n",
      " - 67s - loss: 5.8591e-04 - val_loss: 3.4716e-04\n",
      "Epoch 6/200\n",
      " - 67s - loss: 4.7981e-04 - val_loss: 2.7539e-04\n",
      "Epoch 7/200\n",
      " - 67s - loss: 4.1179e-04 - val_loss: 2.2944e-04\n",
      "Epoch 8/200\n",
      " - 67s - loss: 3.6424e-04 - val_loss: 1.9655e-04\n",
      "Epoch 9/200\n",
      " - 67s - loss: 3.2931e-04 - val_loss: 1.7374e-04\n",
      "Epoch 10/200\n",
      " - 67s - loss: 3.0264e-04 - val_loss: 1.5644e-04\n",
      "Epoch 11/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-2495d712cebe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m history = model.fit(X_train_final, Y_train_final, batch_size=batch_size, epochs=epochs,\n\u001b[0;32m---> 15\u001b[0;31m                       validation_data=(X_dev_final, Y_dev_final), verbose=verbose, shuffle=False) \n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 8192 #16384 #32768 #4096\n",
    "epochs = 200\n",
    "verbose = 2\n",
    "loss = 'mean_squared_error'\n",
    "optimizer = 'adagrad' #'adam'\n",
    "#num_LSTM = 2\n",
    "#n_units = [256, 256]\n",
    "num_LSTM = 3\n",
    "n_units = [256, 256, 256]\n",
    "dropout = 0.1\n",
    "\n",
    "model = initialize_model(X_train_final, loss, optimizer, num_LSTM, n_units, dropout)\n",
    "\n",
    "history = model.fit(X_train_final, Y_train_final, batch_size=batch_size, epochs=epochs,\n",
    "                      validation_data=(X_dev_final, Y_dev_final), verbose=verbose, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = evaluate_model(model, history, X_train_final, X_dev_final, X_test_final, Y_train_final, Y_dev_final, Y_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1221880"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_final.shape[0]*Y_test_final.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.02718626707792282,\n",
       " 0.006338200066238642,\n",
       " 0.0027326869312673807,\n",
       " 0.001698706648312509,\n",
       " 0.001254145405255258,\n",
       " 0.0010413419222459197,\n",
       " 0.0009030214278027415,\n",
       " 0.0008182916790246964,\n",
       " 0.0007581285899505019,\n",
       " 0.0007028873660601676,\n",
       " 0.0006665494875051081,\n",
       " 0.0006137048476375639,\n",
       " 0.0005802287487313151,\n",
       " 0.0005413988837972283,\n",
       " 0.0005202949396334589,\n",
       " 0.0004865903756581247,\n",
       " 0.00047809717943891883,\n",
       " 0.00044235767563804984,\n",
       " 0.00042427933658473194,\n",
       " 0.00040302457637153566,\n",
       " 0.0003817896649707109,\n",
       " 0.00037104933289811015,\n",
       " 0.00035242902231402695,\n",
       " 0.00032933190232142806,\n",
       " 0.0003196826728526503,\n",
       " 0.00030853855423629284,\n",
       " 0.0002918703539762646,\n",
       " 0.0002794977044686675,\n",
       " 0.00026941325631923974,\n",
       " 0.0002592851815279573,\n",
       " 0.00024878818658180535,\n",
       " 0.00024208147078752518,\n",
       " 0.00022939304471947253,\n",
       " 0.0002216892025899142,\n",
       " 0.00021344481501728296,\n",
       " 0.0002067625755444169,\n",
       " 0.0001979375520022586,\n",
       " 0.00019705621525645256,\n",
       " 0.00018816158990375698,\n",
       " 0.0001789871312212199,\n",
       " 0.0001746904745232314,\n",
       " 0.00016753471572883427,\n",
       " 0.0001653828367125243,\n",
       " 0.0001576734648551792,\n",
       " 0.00015177066961769015,\n",
       " 0.0001465666719013825,\n",
       " 0.00014219936565496027,\n",
       " 0.00013979502546135336,\n",
       " 0.00013327923079486936,\n",
       " 0.00012926093768328428,\n",
       " 0.00012493484246078879,\n",
       " 0.00012164339568698779,\n",
       " 0.00011891012400155887,\n",
       " 0.0001152454424300231,\n",
       " 0.00011163418821524829,\n",
       " 0.00010769514483399689,\n",
       " 0.00010648144962033257,\n",
       " 0.0001020264535327442,\n",
       " 0.0001004287987598218,\n",
       " 9.82266356004402e-05,\n",
       " 9.382967255078256e-05,\n",
       " 9.34348427108489e-05,\n",
       " 8.897267980501056e-05,\n",
       " 8.787640399532393e-05,\n",
       " 8.562054426874965e-05,\n",
       " 8.26616887934506e-05,\n",
       " 8.195111877284944e-05,\n",
       " 7.93180224718526e-05,\n",
       " 7.652492786291987e-05,\n",
       " 7.468306284863502e-05,\n",
       " 7.668246689718217e-05,\n",
       " 7.522735540987924e-05,\n",
       " 6.967551598791033e-05,\n",
       " 6.820041744504124e-05,\n",
       " 6.631740689044818e-05,\n",
       " 6.55799376545474e-05,\n",
       " 6.339448736980557e-05,\n",
       " 6.271020538406447e-05,\n",
       " 6.164240767247975e-05,\n",
       " 5.9780933952424675e-05,\n",
       " 5.821609011036344e-05,\n",
       " 5.740811684518121e-05,\n",
       " 5.657842484652065e-05,\n",
       " 5.470053292810917e-05,\n",
       " 5.4033844207879156e-05,\n",
       " 5.291481647873297e-05,\n",
       " 5.216951103648171e-05,\n",
       " 5.118168701301329e-05,\n",
       " 5.051467815064825e-05,\n",
       " 4.958167119184509e-05,\n",
       " 4.8766964027890936e-05,\n",
       " 4.81368915643543e-05,\n",
       " 4.6446581109194085e-05,\n",
       " 4.579684173222631e-05,\n",
       " 4.478845949051902e-05,\n",
       " 4.4341737520881e-05,\n",
       " 4.3501571781234816e-05,\n",
       " 4.301258741179481e-05,\n",
       " 4.2259664041921496e-05,\n",
       " 4.147095751250163e-05,\n",
       " 4.198187161819078e-05,\n",
       " 4.015186277683824e-05,\n",
       " 3.9987127820495516e-05,\n",
       " 3.90567620343063e-05,\n",
       " 3.8604044675594196e-05,\n",
       " 3.800148260779679e-05,\n",
       " 3.763140193768777e-05,\n",
       " 3.73283946828451e-05,\n",
       " 3.6220364563632756e-05,\n",
       " 3.614017623476684e-05,\n",
       " 3.5766151995630935e-05,\n",
       " 3.505568747641519e-05,\n",
       " 3.4515527659095824e-05,\n",
       " 3.3788001019274816e-05,\n",
       " 3.3123909815913066e-05,\n",
       " 3.247809945605695e-05,\n",
       " 3.366628516232595e-05,\n",
       " 3.1937070161802694e-05,\n",
       " 3.132725760224275e-05,\n",
       " 3.1128085538512096e-05,\n",
       " 3.132550045847893e-05,\n",
       " 3.0420136681641452e-05,\n",
       " 2.9667382477782667e-05,\n",
       " 3.089242454734631e-05,\n",
       " 2.957924698421266e-05,\n",
       " 2.8801325242966413e-05,\n",
       " 3.0109706131042913e-05,\n",
       " 2.8552127332659438e-05,\n",
       " 2.7755197152146138e-05,\n",
       " 2.7719665013137273e-05,\n",
       " 2.7433536160970107e-05,\n",
       " 2.6895351766142994e-05,\n",
       " 2.6433743187226355e-05,\n",
       " 2.6682657335186377e-05,\n",
       " 2.5705983716761693e-05,\n",
       " 2.6323017664253712e-05,\n",
       " 2.5510025807307102e-05,\n",
       " 2.5387371351826005e-05,\n",
       " 2.5304658265667967e-05,\n",
       " 2.4543660401832312e-05,\n",
       " 2.4821365514071658e-05,\n",
       " 2.5302024369011633e-05,\n",
       " 2.528813092794735e-05,\n",
       " 2.3373029762296937e-05,\n",
       " 2.459926690789871e-05,\n",
       " 2.299217703694012e-05,\n",
       " 2.4986742573673837e-05,\n",
       " 2.27414166147355e-05,\n",
       " 2.2142627130961046e-05,\n",
       " 2.2029995307093486e-05,\n",
       " 2.189389670093078e-05,\n",
       " 2.1937712517683394e-05,\n",
       " 2.1288078642101027e-05,\n",
       " 2.1078922145534307e-05,\n",
       " 2.154623143724166e-05,\n",
       " 2.1337924408726394e-05,\n",
       " 2.0917199435643852e-05,\n",
       " 2.132302506652195e-05,\n",
       " 2.058757127088029e-05,\n",
       " 2.0473802578635514e-05,\n",
       " 2.020015563175548e-05,\n",
       " 1.949721081473399e-05,\n",
       " 1.9844328562612645e-05,\n",
       " 1.9172968677594326e-05,\n",
       " 1.9176404748577625e-05,\n",
       " 2.0110221157665364e-05,\n",
       " 1.8893484593718313e-05,\n",
       " 2.0151090211584233e-05,\n",
       " 2.1112035028636456e-05,\n",
       " 2.244204188173171e-05,\n",
       " 2.1418331016320735e-05,\n",
       " 1.9035840523429215e-05,\n",
       " 1.8443097360432148e-05,\n",
       " 1.9560106011340395e-05,\n",
       " 1.78139416675549e-05,\n",
       " 2.178339855163358e-05,\n",
       " 1.8306234778719954e-05,\n",
       " 1.6958259948296472e-05,\n",
       " 1.7606573237571865e-05,\n",
       " 1.9494680600473657e-05,\n",
       " 2.0631363440770656e-05,\n",
       " 1.7614196622162126e-05,\n",
       " 1.849471482273657e-05,\n",
       " 1.7369531633448787e-05,\n",
       " 1.6344280084013008e-05,\n",
       " 1.7171889339806512e-05,\n",
       " 1.6143636457854882e-05,\n",
       " 1.5852130673010834e-05,\n",
       " 1.57818849402247e-05,\n",
       " 1.580441494297702e-05,\n",
       " 1.8039701899397187e-05,\n",
       " 1.5380806871689856e-05,\n",
       " 1.691414217930287e-05,\n",
       " 1.5158849237195682e-05,\n",
       " 1.5486024494748563e-05,\n",
       " 1.5186711607384495e-05,\n",
       " 1.5527721188846044e-05,\n",
       " 1.4742112398380414e-05,\n",
       " 1.4391293916560244e-05,\n",
       " 1.6024654541979544e-05]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8192 #16384 #32768 #4096\n",
    "epochs = 200\n",
    "verbose = 2\n",
    "loss = 'mean_squared_error'\n",
    "optimizer = 'adam'\n",
    "#num_LSTM = 2\n",
    "#n_units = [256, 256]\n",
    "num_LSTM = 3\n",
    "n_units = [256, 256, 256]\n",
    "dropout = 0.1\n",
    "\n",
    "model = initialize_model(X_train_final, loss, optimizer, num_LSTM, n_units, dropout)\n",
    "\n",
    "history = model.fit(X_train_final, Y_train_final, batch_size=batch_size, epochs=epochs,\n",
    "                      validation_data=(X_dev_final, Y_dev_final), verbose=verbose, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "loss = 'mse'\n",
    "optimizers = ['adagrad', 'adam', 'rmsprop']\n",
    "batch_sizes = [2048, 8192, 16384]\n",
    "include_monthDayHour = [True, False]\n",
    "num_LSTMs = [2,3]\n",
    "num_units_2 = [[128, 256], [256, 256]]\n",
    "num_units_3 = [[128, 256, 256], [256, 256, 256], [256, 512, 512]]\n",
    "#amount_of_data = {'one_year', 'all'}\n",
    "#dropout = np.random.uniform(0.5, 0.05, num_LSTMs)\n",
    "dropout = 0.1\n",
    "\n",
    "# Load data\n",
    "df = load_data()\n",
    "X_train, X_dev, X_test = split_X(df) \n",
    "Y_train, Y_dev, Y_test = split_Y(df)\n",
    "\n",
    "df_noTime = df.iloc[:,1:16]\n",
    "X_train_noTime, X_dev_noTime, X_test_noTime = split_X(df_noTime) \n",
    "\n",
    "# Initialize output dataframe\n",
    "outfile = 'cboe/grid_search_dataSubset.parquet'\n",
    "columns = ['num_epochs', 'loss', 'optimizer', 'batch_size', 'include_time', 'num_LSTMs', 'num_units',\n",
    "           'train_loss', 'dev_loss', 'test_loss', 'train_prop_correct', 'dev_prop_correct', 'test_prop_correct']\n",
    "df_output = pd.DataFrame(columns=columns)\n",
    "pq.write_table(pa.Table.from_pandas(df_output), outfile, compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for include_time in include_monthDayHour:\n",
    "    if not include_time:\n",
    "        X_train = X_train_noTime\n",
    "        X_dev =   X_dev_noTime\n",
    "        X_test =  X_test_noTime\n",
    "\n",
    "    for optimizer in optimizers:\n",
    "        for batch_size in batch_sizes:\n",
    "            for num_LSTM in num_LSTMs:\n",
    "                if num_LSTM == 2:\n",
    "                    num_units = num_units_2\n",
    "                elif num_LSTM == 3:\n",
    "                    num_units = num_units_3\n",
    "                for n_units in num_units:\n",
    "                    # Load output dataframe\n",
    "                    df_output = pq.read_table(outfile).to_pandas()\n",
    "                    \n",
    "                    # Initialize model\n",
    "                    model = initialize_model(X_train, loss, optimizer, num_LSTM, n_units, dropout)\n",
    "                    \n",
    "                    # Train model\n",
    "                    if count%10==0:\n",
    "                        verbose=1\n",
    "                        print(count, '/', 120)\n",
    "                    else:\n",
    "                        verbose=0\n",
    "                    history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n",
    "                      validation_data=(X_dev, Y_dev), verbose=verbose, shuffle=False) \n",
    "                    \n",
    "                    # Evaluate model\n",
    "                    evaluate = evaluate_model(model, history, X_train, X_dev, X_test, Y_train, Y_dev, Y_test)\n",
    "                    \n",
    "                    # Write to dataframe and save\n",
    "                    row = [num_epochs, loss, optimizer, batch_size, include_time, num_LSTM, str(n_units)]\n",
    "                    row.extend(evaluate)\n",
    "                    df_output.loc[len(df_output)] = row\n",
    "                    df_to_parquet(df_output, outfile)\n",
    "                    \n",
    "                    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2048 batch, 3 cells, numunits[2]\n",
    "# HYPERPARAMETERS\n",
    "#####################\n",
    "loss = 'mean_squared_error'\n",
    "optimizer = 'adagrad'\n",
    "batch_size = 2048\n",
    "epochs = 10\n",
    "\n",
    "num_LSTMs = 3\n",
    "num_units = num_units_3[2]\n",
    "model = initialize_model(X_train, loss, optimizer, num_LSTMs, num_units, dropout)\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n",
    "                      validation_data=(X_dev, Y_dev), verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
